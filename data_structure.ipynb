{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data file structures and sets:\n",
    "\n",
    "This script creates csv files with file names and labels for all used data sets.\n",
    "The tensorflow pipeline reads the csv files later and imports images in batches.\n",
    "\n",
    "Used data sets:\n",
    "    - intrinsic imigas in the wild (iiw)\n",
    "    - MPI Sintel data set\n",
    "    - MIT data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-27T10:15:38.916585Z",
     "start_time": "2017-05-27T10:15:38.911697Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-27T10:28:51.788166Z",
     "start_time": "2017-05-27T10:28:51.784393Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# directory where to save csv files:\n",
    "save_csvs = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-27T10:15:40.177933Z",
     "start_time": "2017-05-27T10:15:40.151425Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_datasets(df, p_train, p_valid, p_test):\n",
    "    \"\"\"\n",
    "    Splits a data set df into training, validation and testing data set with relative cardinality\n",
    "    p_train, p_valid and p_test, respectively.\n",
    "    :param df: complete data set which should be split into training, validation and testing sets\n",
    "    :type df: pd.DataFrame()\n",
    "    :param p_train: relative cardinality of training data set\n",
    "    :type p_train: float (\\elem [0,1])\n",
    "    :param p_valid: relative cardinality of validation data set\n",
    "    :type p_valid: float (\\elem [0,1])\n",
    "    :param p_test: relative cardinality of testing data set\n",
    "    :type p_test: float (\\elem [0,1])\n",
    "    :return: training, validation and testing data sets\n",
    "    :type: [pd.DataFrame, pd.DataFrame, pd.DataFrame]\n",
    "    \"\"\"\n",
    "    # make sure we have consistancy:\n",
    "    assert p_train + p_valid + p_test  == 1, 'p_train, p_valid, p_test must add up to 1'\n",
    "    # this data set will be the training data set in the end:\n",
    "    df_train = df.copy()\n",
    "    # sampling data to get testing set:\n",
    "    df_test = df_train.sample(n=int(p_test * df.shape[0]), frac=None, replace=False, weights=None, random_state=42, axis=0)\n",
    "    # drop these sampled test data (we do not want them in the other data sets):\n",
    "    df_train.drop(df_test.index, inplace=True)\n",
    "    # sampling data to get validation set:\n",
    "    df_valid = df_train.sample(n=int(p_valid * df.shape[0]), frac=None, replace=False, weights=None, random_state=42, axis=0)\n",
    "    # drop these sampled valid data (we do not want them in the training set):\n",
    "    df_train.drop(df_valid.index, inplace=True)\n",
    "    return df_train, df_valid, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set iiw:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-27T10:28:52.549776Z",
     "start_time": "2017-05-27T10:28:52.545503Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# directory of data\n",
    "data_dir_iiw = 'data/iiw-dataset/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-27T11:07:11.903216Z",
     "start_time": "2017-05-27T11:07:11.154907Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import file names of data directory:\n",
    "df_iiw = pd.DataFrame([[int(os.path.splitext(os.path.basename(x))[0]), \n",
    "                        os.path.relpath(x, save_csvs), \n",
    "                        os.path.splitext(os.path.relpath(x, save_csvs))[0]+'.json'] for x in glob.glob(data_dir_iiw + '/*.png')], \n",
    "                      columns=['file_id', 'image_path', 'label_path'])\n",
    "# sort by file ids (we can sort these files because they are shuffled during training in tf anyways):\n",
    "df_iiw.sort_values(by='file_id', inplace=True)\n",
    "# reset indices of pd.DataFrame:\n",
    "df_iiw.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-27T11:07:35.963700Z",
     "start_time": "2017-05-27T11:07:35.953606Z"
    }
   },
   "outputs": [],
   "source": [
    "# get training validation and testing data set of the iiw data:\n",
    "df_iiw_train, df_iiw_valid, df_iiw_test = create_datasets(df=df_iiw, p_train=0.8, p_valid=0.1, p_test=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-27T11:07:48.297052Z",
     "start_time": "2017-05-27T11:07:48.242487Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save complete data set, training data set, validation data set and testing data set in separate data files:\n",
    "df_iiw.to_csv(path_or_buf=save_csvs + 'data_iiw_complete.csv', sep=',', columns=['image_path', 'label_path'], index=False, header=False)\n",
    "df_iiw_train.to_csv(path_or_buf=save_csvs + 'data_iiw_train.csv', sep=',', columns=['image_path', 'label_path'], index=False, header=False)\n",
    "df_iiw_valid.to_csv(path_or_buf=save_csvs + 'data_iiw_valid.csv', sep=',', columns=['image_path', 'label_path'], index=False, header=False)\n",
    "df_iiw_test.to_csv(path_or_buf=save_csvs + 'data_iiw_test.csv', sep=',', columns=['image_path', 'label_path'], index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sintel data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-27T10:26:23.374192Z",
     "start_time": "2017-05-27T10:26:23.370439Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir_sintel = 'data/mpi-sintel-complete/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-27T11:28:54.647228Z",
     "start_time": "2017-05-27T11:28:54.482828Z"
    }
   },
   "outputs": [],
   "source": [
    "# use 'clean pass' images (see narihira2015: p.3: \"'final images' [...] are the result of additional computer \n",
    "# graphics tricks which dristract from our application.\"):\n",
    "df_sintel = pd.DataFrame([[os.path.relpath(x, save_csvs), \n",
    "                           os.path.relpath(x, save_csvs).replace('clean', 'albedo')\n",
    "                          ] for x in glob.glob(data_dir_sintel + 'training/clean/**/*.png')],\n",
    "                         columns=['image_path', 'label_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-27T11:30:39.578624Z",
     "start_time": "2017-05-27T11:30:39.569867Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get training validation and testing data set of the mpi-sintel data:\n",
    "df_sintel_train, df_sintel_valid, df_sintel_test = create_datasets(df=df_sintel, p_train=0.8, p_valid=0.1, p_test=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-27T11:32:20.530932Z",
     "start_time": "2017-05-27T11:32:20.491200Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save complete data set, training data set, validation data set and testing data set in separate data files:\n",
    "df_sintel.to_csv(path_or_buf=save_csvs + 'data_sintel_complete.csv', sep=',',\n",
    "                 columns=['image_path', 'label_path'], index=False, header=False)\n",
    "df_sintel_train.to_csv(path_or_buf=save_csvs + 'data_sintel_train.csv', sep=',',\n",
    "                       columns=['image_path', 'label_path'], index=False, header=False)\n",
    "df_sintel_valid.to_csv(path_or_buf=save_csvs + 'data_sintel_valid.csv', sep=',', \n",
    "                       columns=['image_path', 'label_path'], index=False, header=False)\n",
    "df_sintel_test.to_csv(path_or_buf=save_csvs + 'data_sintel_test.csv', sep=',', \n",
    "                      columns=['image_path', 'label_path'], index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-27T11:42:20.171541Z",
     "start_time": "2017-05-27T11:42:20.111972Z"
    }
   },
   "outputs": [],
   "source": [
    "# also save (unknown) test files:\n",
    "df_sintel_test_unknown = pd.DataFrame([[os.path.relpath(x, save_csvs),\n",
    "                                        None\n",
    "                                       ] for x in glob.glob(data_dir_sintel + 'test/clean/**/*.png')],\n",
    "                                      columns=['image_path', 'label_path'])\n",
    "df_sintel_test_unknown.to_csv(path_or_buf=save_csvs + 'data_sintel_test_unknown.csv', sep=',',\n",
    "                              columns=['image_path', 'label_path'], index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIT data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-27T11:34:15.187433Z",
     "start_time": "2017-05-27T11:34:15.183854Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir_mit = 'data/mit_intrinsic/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-27T11:57:14.577544Z",
     "start_time": "2017-05-27T11:57:14.562519Z"
    }
   },
   "outputs": [],
   "source": [
    "df_mit = pd.DataFrame([[os.path.relpath(x, save_csvs), \n",
    "                        os.path.relpath(x, save_csvs).replace('original', 'reflectance'), \n",
    "                        os.path.relpath(x, save_csvs).replace('original', 'shading')\n",
    "                       ] for x in glob.glob(data_dir_mit + '**/original.png')],\n",
    "                      columns=['image_path', 'albedo_label_path', 'shading_label_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-27T11:58:13.667006Z",
     "start_time": "2017-05-27T11:58:13.659425Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get training validation and testing data set of the mit data:\n",
    "df_mit_train, df_mit_valid, df_mit_test = create_datasets(df=df_mit, p_train=0.8, p_valid=0.1, p_test=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-27T12:02:17.493047Z",
     "start_time": "2017-05-27T12:02:17.473422Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save complete data set, training data set, validation data set and testing data set in separate data files:\n",
    "df_mit.to_csv(path_or_buf=save_csvs + 'data_mit_complete.csv', sep=',', \n",
    "              columns=['image_path', 'albedo_label_path', 'shading_label_path'], index=False, header=False)\n",
    "df_mit_train.to_csv(path_or_buf=save_csvs + 'data_mit_train.csv', sep=',', \n",
    "                    columns=['image_path', 'albedo_label_path', 'shading_label_path'], index=False, header=False)\n",
    "df_mit_valid.to_csv(path_or_buf=save_csvs + 'data_mit_valid.csv', sep=',', \n",
    "                    columns=['image_path', 'albedo_label_path', 'shading_label_path'], index=False, header=False)\n",
    "df_mit_test.to_csv(path_or_buf=save_csvs + 'data_mit_test.csv', sep=',', \n",
    "                   columns=['image_path', 'albedo_label_path', 'shading_label_path'], index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "84px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
