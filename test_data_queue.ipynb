{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Test data queue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-08T15:54:35.047036Z",
     "start_time": "2017-07-08T15:54:31.956384Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: \n",
      "3.6.0 (default, Dec 24 2016, 13:33:34) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)]\n",
      "Tensorflow version: \n",
      "1.2.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./util')\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import input_queues as iq\n",
    "import cnn_model\n",
    "import plot_helpers as plt_help\n",
    "import general_helpers as ghelp\n",
    "import cnn_helpers as cnnhelp\n",
    "\n",
    "print('Python version: \\n' + sys.version)\n",
    "print('Tensorflow version: \\n' + tf.__version__)\n",
    "\n",
    "# data path constants:\n",
    "# DATA_DIR = '../data/mnist/'\n",
    "DATA_DIR = 'data/'\n",
    "PREDICT_PATH = ''\n",
    "path_inference_graph = ['logs/inference_graphs/narihira2015/' +\n",
    "                        'tfmodel_inference.meta']\n",
    "# path_inference_graph = ['/Users/udodehm/Downloads/camp_depth_irolaina/ResNet_pretrained/ResNet-L50.meta']\n",
    "# path_inference_graph = ['vgg16/vgg16.tfmodel']\n",
    "\n",
    "path_inference_graph = path_inference_graph[0]\n",
    "path_restore_model = None #'logs/2/tfmodel-5'\n",
    "LOGS_PATH = 'logs/1/'  # path to summary files\n",
    "\n",
    "# hyper-parameters:\n",
    "m_height = 13  # multiplicate of image height size -> network is designed so \n",
    "    # that it can take images with shape of multiples of m\n",
    "m_width = m_height  # multiplicate of image width size -> network \n",
    "    # is designed so that it can take images with shape of multiples of m\n",
    "IMAGE_SHAPE = [32 * m_height, 32 * m_width, 3]  # complete image size \n",
    "    # [436, 1024, 3] # Narihira2015 use [M*32=13*32=416, 416, 3]\n",
    "INITIAL_LEARNING_RATE = 1e-5\n",
    "# probability that a neuron's output is kept during dropout (only during \n",
    "# training!!!, testing/validation -> 1.0):\n",
    "# DROPOUT_RATE = 0.5\n",
    "BATCH_SIZE = 8  # nr of data which is put through the network before updating \n",
    "    # it, as default use: 32. \n",
    "# BATCH_SIZE determines how many data samples are loaded in the memory (be \n",
    "# careful with memory space)\n",
    "NUM_EPOCHS = 3  # nr of times the training process loops through the \n",
    "    # complete training data set (how often is the tr set 'seen')\n",
    "    # if you have 1000 training examples, and your batch size is 500, then it\n",
    "    # will take 2 iterations to complete 1 epoch.\n",
    "\n",
    "DISPLAY_STEP = 2  # every DIPLAY_STEP'th training iteration information is \n",
    "    # printed (default: 100)\n",
    "SUMMARY_STEP = 2  # every SUMMARY_STEP'th training iteration a summary file is \n",
    "    # written to LOGS_PATH\n",
    "DEVICE = '/cpu:0'  # device on which the variable is saved/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-08T16:02:00.394752Z",
     "start_time": "2017-07-08T16:01:57.751814Z"
    },
    "code_folding": [],
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('data'):\n",
    "    # import training data set\n",
    "    file = 'sample_data_sintel_shading_train.csv'\n",
    "    data_train = iq.SintelDataInputQueue(path_csv_file = DATA_DIR + file,\n",
    "                                         batch_size=BATCH_SIZE, \n",
    "                                         num_epochs=NUM_EPOCHS, \n",
    "                                         nr_data=None)\n",
    "    # if data_augmentation=True: images are randomly rotated in range (-15, 15) \n",
    "    # deg and randomly horizontally flipped:\n",
    "    data_train_out = data_train.next_batch(image_shape=IMAGE_SHAPE, \n",
    "                                data_augmentation=True)\n",
    "    _, _, _, imgs_batch_tr, albedo_batch_tr, shading_batch_tr = data_train_out\n",
    "\n",
    "    # import validation data set: \n",
    "    # why not using the whole validation set for validation at once? \n",
    "    # - limited memory space.\n",
    "    #  -> After each training epoch we will use the complete validation dataset\n",
    "    #     to calculate the error/accuracy on the validation set\n",
    "    file = 'sample_data_sintel_shading_valid.csv'\n",
    "    data_valid = iq.SintelDataInputQueue(path_csv_file = DATA_DIR + file,\n",
    "                                         batch_size=5, \n",
    "                                         num_epochs=NUM_EPOCHS,\n",
    "                                         nr_data=None)\n",
    "    # if data_augmentation=True: images are randomly rotated in range (-15, 15) \n",
    "    # deg and randomly horizontally flipped:\n",
    "    data_val_out = data_valid.next_batch(image_shape=IMAGE_SHAPE,\n",
    "                                         data_augmentation=False)\n",
    "    _, _, _, imgs_batch_val, albedo_batch_val, shading_batch_val = data_val_out\n",
    "\n",
    "    \n",
    "    # testing data set: \n",
    "    file = 'sample_data_sintel_shading_test.csv'\n",
    "    data_test = iq.SintelDataInputQueue(path_csv_file = DATA_DIR + file,\n",
    "                                        batch_size=16, \n",
    "                                        num_epochs=NUM_EPOCHS, \n",
    "                                        nr_data=None)\n",
    "    # if data_augmentation=True: images are randomly rotated in range (-15, 15)\n",
    "    # deg and randomly horizontally flipped:\n",
    "    data_te_out = data_test.next_batch(image_shape=IMAGE_SHAPE,\n",
    "                                       data_augmentation=False)\n",
    "    image_path_batch_test, albedo_path_batch_test, shading_path_batch_test, \\\n",
    "        images_batch_test, albedo_batch_test, shading_batch_test = data_te_out\n",
    "    \n",
    "    # for the test set create also \n",
    "    image_path_test, albedo_label_path_test, shading_label_path_test = data_test.read_csv_file(record_defaults=[[''], [''], ['']])\n",
    "    \n",
    "    images_test = data_test.read_image(image_path=image_path_test)\n",
    "    labels_albedo_test = data_test.read_image(image_path=albedo_label_path_test)\n",
    "    labels_shading_test = data_test.read_image(image_path=shading_label_path_test)\n",
    "    images_test, labels_albedo_test, labels_shading_test = data_test.random_crop_images_and_labels(image_and_labels=[images_test,\n",
    "                                                                                                                     labels_albedo_test,\n",
    "                                                                                                                     labels_shading_test],\n",
    "                                                                                                   channels=[IMAGE_SHAPE[-1]]*3,\n",
    "                                                                                                   spatial_shape=IMAGE_SHAPE[:2],\n",
    "                                                                                                   data_augmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-08T16:23:37.853767Z",
     "start_time": "2017-07-08T16:23:17.570975Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be str, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-db0ec7338ab0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     iters = int(data_test.nr_data / data_test.batch_size * \n\u001b[1;32m     23\u001b[0m                       data_test.num_epochs)\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'available iterations: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0miters_used\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'used iterations: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0miters_used\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be str, not int"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Run call was cancelled\n"
     ]
    }
   ],
   "source": [
    "# with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    ############################################################################\n",
    "    # Initialization:\n",
    "    # Op that initializes global variables in the graph:\n",
    "    init_global = tf.global_variables_initializer()\n",
    "    # Op that initializes local variables in the graph:\n",
    "    init_local = tf.local_variables_initializer()\n",
    "    # initialize all variables:\n",
    "    sess.run([init_global, init_local])\n",
    "    # start timer for total training time:\n",
    "    start_total_time = time.time()\n",
    "    # set timer to measure the displayed training steps:\n",
    "    start_time = start_total_time\n",
    "    \n",
    "    # Start populating the filename queue.\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    \n",
    "    iters = int(data_test.nr_data / data_test.batch_size * \n",
    "                      data_test.num_epochs)\n",
    "    print('available iterations: ' + iters)\n",
    "    iters_used = int(NUM_EPOCHS * 0.75)\n",
    "    print('used iterations: ' + iters_used)\n",
    "    \n",
    "    for i in range(iters_used):\n",
    "        nxt_img_path_batch_te, nxt_albedo_path_batch_te, nxt_shad_path_batch_te, \\\n",
    "            nxt_img_batch_te, nxt_albedo_batch_te, nxt_shad_batch_te = \\\n",
    "                sess.run([image_path_batch_test, albedo_path_batch_test, \n",
    "                          shading_path_batch_test, images_batch_test, \n",
    "                          albedo_batch_test, shading_batch_test])\n",
    "        print(nxt_img_path_batch_te)\n",
    "#     ############################################################################\n",
    "#     # Training:\n",
    "#     count_epoch = 0\n",
    "#     # train loop\n",
    "#     #     train for until all data is used\n",
    "#     #     number of iterations depends on number of data, number of epochs and \n",
    "#     #     batch size:\n",
    "#     train_iters = int(data_train.nr_data / data_train.batch_size * \n",
    "#                       data_train.num_epochs)\n",
    "\n",
    "#     print('INFO: For training it takes {} '.format(train_iters) +\n",
    "#           '(= # data / batch_size * epochs) iterations to loop through ' +\n",
    "#           '{} samples of training data over '.format(data_train.nr_data) +\n",
    "#           '{} epochs summarized in batches '.format(data_train.num_epochs) +\n",
    "#           'of size {}.\\n'.format(data_train.batch_size) +\n",
    "#           'So, there are # data / batch_size = ' +\n",
    "#           '{} '.format(int(data_train.nr_data / data_train.batch_size)) + \n",
    "#           'iterations per epoch.\\n')\n",
    "    \n",
    "#     for i in range(train_iters):\n",
    "#         # take a (mini) batch of the training data:\n",
    "#         # method of the DataSet class\n",
    "#         lst = [imgs_batch_tr, albedo_batch_tr, shading_batch_tr]\n",
    "#         next_imgs_batch, next_albedo_batch, next_shad_batch = sess.run(lst)\n",
    "        \n",
    "    \n",
    "#     end_total_time = time.time() - start_total_time\n",
    "#     end_total_time = ghelp.get_time_format(end_total_time)\n",
    "#     print('\\nTraining done... total training time: ' + \n",
    "#           '{h:02}:'.format(h=end_total_time[0]) +\n",
    "#           '{m:02}:{s:02} h.'.format(m=end_total_time[1], s=end_total_time[2]))\n",
    "    \n",
    "    ############################################################################\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-08T16:14:17.613803Z",
     "start_time": "2017-07-08T16:14:17.519676Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-08T16:14:37.329433Z",
     "start_time": "2017-07-08T16:14:37.320792Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.75"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.nr_data / data_test.batch_size * data_test.num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "notify_time": "5",
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
