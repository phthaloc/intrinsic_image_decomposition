{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Import inference graph and extend it to a training graph:\n",
    "\n",
    "To extend the graph, we have to add:\n",
    "    - input data structure\n",
    "    - loss function\n",
    "    - optimization op\n",
    "    \n",
    "To build the graph it is necessary to know the node names of all relevant \n",
    "layers, placeholders etc. for restoring the model later.\n",
    "\n",
    "pay ATTENTION to:\n",
    "    the imported model must be in the same graph as the nodes which are \n",
    "    added later\n",
    "    -> load the model first to the default graph, then add further ops\n",
    "    \n",
    "also test to input input-images of different sizes (multiples of 32px). \n",
    "It might not work because input placeholder is defined fix.\n",
    "perhaps input node should not be saved inside the model?!\n",
    "\n",
    "output of model has name (deconv_s2out_shading/BiasAdd:0 and \n",
    "deconv_s2out_albedo/BiasAdd:0). \n",
    "simpler names?!\n",
    "\n",
    "\n",
    "To plot all graphs directly in this notebook, run jupyter form terminal like \n",
    "this:\n",
    "    jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-31T14:05:02.277830Z",
     "start_time": "2017-08-31T14:05:00.568554Z"
    }
   },
   "outputs": [],
   "source": [
    "import os   \n",
    "import sys\n",
    "sys.path.append('./util')\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import input_queues as iq\n",
    "import cnn_model\n",
    "import plot_helpers as plt_help\n",
    "import general_helpers as ghelp\n",
    "import nn_helpers as nnhelp\n",
    "import cnn_helpers as cnnhelp\n",
    "import download\n",
    "\n",
    "# make only 'gpu:0' visible, so that only one gpu is used not both, see also\n",
    "# https://github.com/tensorflow/tensorflow/issues/5066\n",
    "# https://github.com/tensorflow/tensorflow/issues/3644#issuecomment-237631171\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "def train_network(LOGS_PATH, DATA_DIR, path_inference_graph, path_restore_model,\n",
    "                  restore_scope, IMAGE_SHAPE, INITIAL_LEARNING_RATE, loss_opt, \n",
    "                  BATCH_SIZE, NUM_EPOCHS, DISPLAY_STEP, SAVE_STEP, nodes_name_dict,\n",
    "                  plot_inference_graph=False, is_sample_size=False):\n",
    "    \"\"\"\n",
    "    :param LOGS_PATH: path to summary files\n",
    "    :param DATA_DIR: data path\n",
    "    :param path_inference_graph:\n",
    "    :param path_restore_model: e.g. 'logs/2/tfmodel-5' or None\n",
    "    :param IMAGE_SHAPE: complete image size [436, 1024, 3]\n",
    "        Narihira2015 use [M*32=13*32=416, 416, 3]\n",
    "    :param INITIAL_LEARNING_RATE: hyper-parameters\n",
    "    :param loss_opt:\n",
    "    :param BATCH_SIZE: nr of data which is put through the network before \n",
    "        updating it, as default use: 16, 32 or 64. \n",
    "        BATCH_SIZE determines how many data samples are loaded in the memory \n",
    "        (be careful with memory space)\n",
    "    :param NUM_EPOCHS: nr of times the training process loops through the \n",
    "        complete training data set (how often is the tr set 'seen')\n",
    "        if you have 1000 training examples, and your batch size is 500, then it\n",
    "        will take 2 iterations to complete 1 epoch.\n",
    "    :param DISPLAY_STEP: every DIPLAY_STEP'th training iteration information is \n",
    "        printed (default: 100)\n",
    "    :param SAVE_STEP: every SAVE_STEP'th training iteration a summary file is \n",
    "        written to LOGS_PATH and checkpoint files are saved\n",
    "    :param plot_inference_graph: flag, True if inference graph should be \n",
    "        plotted (default: False).\n",
    "    :param is_sample_size: flag, if True only a smaller sample size is used for \n",
    "        training and validation.\n",
    "    \"\"\"\n",
    "    ############################################################################\n",
    "\n",
    "    # create logger (write to file and stdout):\n",
    "    logger = ghelp.create_logger(filename=LOGS_PATH + 'training.log')\n",
    "    logger.debug('Python version: \\n    ' + sys.version + \n",
    "                 '\\n    Tensorflow version: ' + tf.__version__)\n",
    "    logger.info('Training on images of shape: {}'.format(IMAGE_SHAPE))\n",
    "    logger.info('Initial learning rate: {}'.format(INITIAL_LEARNING_RATE))\n",
    "    logger.info('Loss function used for optimization: {}'.format(loss_opt))\n",
    "    logger.info('Batch size: {}'.format(BATCH_SIZE))\n",
    "    logger.info('# epochs: {}'.format(NUM_EPOCHS))\n",
    "    logger.info('Report training set loss every {}'.format(DISPLAY_STEP) +\n",
    "                'iteration.')\n",
    "    logger.info('Write summary and checkpoints to file every ' +\n",
    "                '{}-th iteration.'.format(SAVE_STEP))\n",
    "\n",
    "    # load meta graph (inference graph)\n",
    "    # how to work with restored models:\n",
    "    # https://www.tensorflow.org/programmers_guide/meta_graph\n",
    "    # http://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/\n",
    "    saver_restore = tf.train.import_meta_graph(path_inference_graph, \n",
    "                                               clear_devices=True)\n",
    "\n",
    "    logger.debug('Restored inference graph from\\n' +\n",
    "                 '    {}'.format(path_inference_graph))\n",
    "    variables_to_restore = slim.get_variables_to_restore(include=[restore_scope],\n",
    "                                                         exclude=None)\n",
    "    logger.info('# of parameters that can be restored: ' + \n",
    "                '{}.'.format(len(variables_to_restore)))\n",
    "\n",
    "    ############################################################################\n",
    "\n",
    "    # save default graph in variable:\n",
    "    graph = tf.get_default_graph()\n",
    "    if plot_inference_graph:\n",
    "        # plot imported inference graph:\n",
    "        plt_help.show_graph(graph.as_graph_def())\n",
    "\n",
    "    # lets get the input\n",
    "    x = graph.get_tensor_by_name(name=nodes_name_dict['input'])\n",
    "\n",
    "    # setup target output classes (ground truth):\n",
    "    y_albedo_label = tf.placeholder(dtype=tf.float32, \n",
    "                                    shape=[None] + IMAGE_SHAPE, \n",
    "                                    name='out_albedo')\n",
    "    y_shading_label = tf.placeholder(dtype=tf.float32, \n",
    "                                     shape=[None] + IMAGE_SHAPE, \n",
    "                                     name='out_shading')\n",
    "\n",
    "    # bool variable that indicates if we are in training mode (training=True) or\n",
    "    # valid/test mode (training=False) this indicator is important if dropout \n",
    "    # or/and batch normalization is used.\n",
    "    try:\n",
    "        # try importing training node (is needed for models that use batch \n",
    "        # normalization etc.)\n",
    "        training = graph.get_tensor_by_name(name='is_training:0')\n",
    "    except KeyError:\n",
    "        # elsewise just define a placeholder wich is used as dummy variable\n",
    "        # and won't be used later:\n",
    "        training = tf.placeholder(dtype=tf.bool, name='is_training')\n",
    "\n",
    "    invalid_px_mask = tf.placeholder(dtype=tf.float32, \n",
    "                                     shape=[None] + IMAGE_SHAPE, \n",
    "                                     name='invalid_px_mask')\n",
    "\n",
    "    # get graph output nodes:\n",
    "    y_albedo_pred = graph.get_tensor_by_name(name=nodes_name_dict['output_albedo'])\n",
    "    y_shading_pred = graph.get_tensor_by_name(name=nodes_name_dict['output_shading'])\n",
    "    # y_albedo_pred = tf.clip_by_value(t=y_albedo, clip_value_min=0, \n",
    "    #                                  clip_value_max=1, \n",
    "    #                                  name='0_1_clipping_albedo')\n",
    "    # y_shading_pred = tf.clip_by_value(t=y_shading, clip_value_min=0,\n",
    "    #                                   clip_value_max=1, \n",
    "    #                                   name='0_1_clipping_shading')\n",
    "\n",
    "    ############################################################################\n",
    "    ############################################################################\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        valid_mask = cnnhelp.get_valid_pixels(image=x, \n",
    "                                              invalid_mask=invalid_px_mask)\n",
    "        loss_type, lambda_ = 'berhu', None\n",
    "        loss_berhu = cnnhelp.loss_fct(label_albedo=y_albedo_label,\n",
    "                                      label_shading=y_shading_label,\n",
    "                                      prediction_albedo=y_albedo_pred, \n",
    "                                      prediction_shading=y_shading_pred,\n",
    "                                      lambda_=lambda_,\n",
    "                                      loss_type=loss_type, \n",
    "                                      valid_mask=valid_mask, \n",
    "                                      log=True)\n",
    "\n",
    "        loss_type, lambda_ = 'mse', 0\n",
    "        loss_l2 = cnnhelp.loss_fct(label_albedo=y_albedo_label,\n",
    "                                   label_shading=y_shading_label,\n",
    "                                   prediction_albedo=y_albedo_pred, \n",
    "                                   prediction_shading=y_shading_pred,\n",
    "                                   lambda_=lambda_,\n",
    "                                   loss_type=loss_type, \n",
    "                                   valid_mask=valid_mask, \n",
    "                                   log=True)\n",
    "\n",
    "        loss_type, lambda_ = 'mse', 1\n",
    "        loss_l2_invariant = cnnhelp.loss_fct(label_albedo=y_albedo_label,\n",
    "                                             label_shading=y_shading_label,\n",
    "                                             prediction_albedo=y_albedo_pred, \n",
    "                                             prediction_shading=y_shading_pred,\n",
    "                                             lambda_=lambda_,\n",
    "                                             loss_type=loss_type, \n",
    "                                             valid_mask=valid_mask, \n",
    "                                             log=True)\n",
    "\n",
    "        loss_type, lambda_ = 'mse', 0.5\n",
    "        loss_l2_avg = cnnhelp.loss_fct(label_albedo=y_albedo_label,\n",
    "                                       label_shading=y_shading_label,\n",
    "                                       prediction_albedo=y_albedo_pred, \n",
    "                                       prediction_shading=y_shading_pred,\n",
    "                                       lambda_=lambda_,\n",
    "                                       loss_type=loss_type, \n",
    "                                       valid_mask=valid_mask, \n",
    "                                       log=True)\n",
    "\n",
    "        loss_dict = {'berhu': loss_berhu,\n",
    "                     'l2': loss_l2,\n",
    "                     'l2_invariant': loss_l2_invariant,\n",
    "                     'l2_avg': loss_l2_avg}\n",
    "\n",
    "        loss = loss_dict[loss_opt]\n",
    "    logger.debug('Defined losses.')\n",
    "\n",
    "    ############################################################################\n",
    "\n",
    "    # Use an AdamOptimizer to train the network:\n",
    "    with tf.name_scope('optimization'):\n",
    "        opt_step = tf.train.AdamOptimizer(INITIAL_LEARNING_RATE).minimize(loss)\n",
    "    logger.debug('Defined optimization method.')\n",
    "\n",
    "    ############################################################################\n",
    "\n",
    "    # to get every summary defined above we merge them to get one target:\n",
    "    merge_train_summaries = tf.summary.merge_all()\n",
    "    # define a FileWriter op which writes summaries defined above to disk:\n",
    "    summary_writer = tf.summary.FileWriter(LOGS_PATH)\n",
    "    # Create a saver for writing training checkpoints.\n",
    "    saver = tf.train.Saver(max_to_keep=NUM_EPOCHS)\n",
    "\n",
    "    ############################################################################\n",
    "\n",
    "    # introduce some validation set specific summaries\n",
    "    # These summaries need to be defined blow the function \n",
    "    # merge_train_summaries = tf.summary.merge_all()\n",
    "    # because the validation set summaries are added to the summary writer at \n",
    "    # different times. If they had been summarized with the training summaries \n",
    "    # they would have to be defined at times where merge_train_summaries are \n",
    "    # added to the summary writer\n",
    "    with tf.name_scope('loss/valid/'):\n",
    "        valid_dict = {key: tf.placeholder(dtype=tf.float32, name=key) for \\\n",
    "                      key in loss_dict.keys()}\n",
    "        valid_summaries = [tf.summary.scalar(name=key, tensor=val) for \\\n",
    "                           key, val in valid_dict.items()]\n",
    "        valid_sums_merged = tf.summary.merge(inputs=valid_summaries,\n",
    "                                             collections=None, name=None)\n",
    "    logger.debug('Defined validation losses.')\n",
    "    logger.debug('Finished building training graph.')\n",
    "    logger.info('Total parameters of network: ' +\n",
    "                '{}'.format(nnhelp.network_params()))\n",
    "\n",
    "    ############################################################################\n",
    "    ############################################################################\n",
    "\n",
    "    # import data:\n",
    "    if is_sample_size:\n",
    "        sample = 'sample_'\n",
    "    else:\n",
    "        sample = ''\n",
    "    # import training data:\n",
    "    file = sample + 'data_sintel_shading_train.csv'\n",
    "    df_train = pd.read_csv(DATA_DIR + file, sep=',', header=None,\n",
    "                           names=['img', 'alb', 'shad', 'invalid'])\n",
    "    # complete image paths:\n",
    "    df_train = DATA_DIR + df_train\n",
    "\n",
    "    # # enable this line to train on only one image:\n",
    "    # df_train1 = df_train.loc[[0]]\n",
    "    # # replicate this row 100 times:\n",
    "    # df_train = pd.concat([df_train1]*100).reset_index(drop=True)\n",
    "\n",
    "    # instantiate a data queue for feeding data in (mini) batches to cnn:\n",
    "    data_train = iq.DataQueue(df=df_train, batch_size=BATCH_SIZE,\n",
    "                              num_epochs=NUM_EPOCHS)\n",
    "    logger.debug('Imported training data ' + \n",
    "                 '(#: {}) '.format(data_train.df.shape[0]) + \n",
    "                 'from\\n    {}'.format(DATA_DIR + file))\n",
    "\n",
    "    # import validation data set: \n",
    "    # why not using the whole validation set for validation at once? \n",
    "    # - limited memory space.\n",
    "    #  -> After each training epoch we will use the complete validation dataset\n",
    "    #     to calculate the error/accuracy on the validation set\n",
    "    file = sample + 'data_sintel_shading_valid.csv'\n",
    "    df_valid = pd.read_csv(DATA_DIR + file, sep=',', header=None,\n",
    "                           names=['img', 'alb', 'shad', 'invalid'])\n",
    "    # complete image paths:\n",
    "    df_valid = DATA_DIR + df_valid\n",
    "    # instantiate a data queue for feeding data in (mini) batches to cnn:\n",
    "    data_valid = iq.DataQueue(df=df_valid, batch_size=BATCH_SIZE,\n",
    "                              num_epochs=NUM_EPOCHS)\n",
    "    logger.debug('Imported validation data ' +\n",
    "                 '(#: {}) '.format(data_valid.df.shape[0]) + \n",
    "                 'from\\n    {}'.format(DATA_DIR + file))\n",
    "\n",
    "    ############################################################################\n",
    "    ############################################################################\n",
    "\n",
    "    logger.info('Start training:')\n",
    "    # Initialization:\n",
    "    # Op that initializes global variables in the graph:\n",
    "    init_global = tf.global_variables_initializer()\n",
    "    # Op that initializes local variables in the graph:\n",
    "    init_local = tf.local_variables_initializer()\n",
    "\n",
    "    config = tf.ConfigProto(device_count = {'GPU': 1},\n",
    "                            intra_op_parallelism_threads=3\n",
    "    #                        allow_soft_placement = True,\n",
    "    #                        intra_op_parallelism_threads=3,\n",
    "    #                        log_device_placement=False\n",
    "                           )\n",
    "    with tf.Session(config=config) as sess: \n",
    "        ########################################################################\n",
    "        # initialize all variables:\n",
    "        sess.run([init_global, init_local])\n",
    "\n",
    "        if path_restore_model:\n",
    "            try:\n",
    "                # restore saved model parameters (weights, biases, etc):\n",
    "                saver_restore.restore(sess, path_restore_model)\n",
    "                logger.info('Restoring parameters from ' +\n",
    "                            '{}'.format(path_restore_model))\n",
    "            except (tf.errors.InvalidArgumentError, tf.errors.NotFoundError):\n",
    "                # in the worst case parameters are loaded twice.\n",
    "                # restore the parameters:\n",
    "                init_fn = slim.assign_from_checkpoint_fn(path_restore_model,\n",
    "                                                         variables_to_restore)\n",
    "                init_fn(sess)\n",
    "                logger.info('Restoring parameters from ' +\n",
    "                            '{}'.format(path_restore_model))\n",
    "\n",
    "        # Adds a Graph to the event file.\n",
    "        # create summary that give output (TensorFlow op that output protocol \n",
    "        # buffers containing 'summarized' data) of the built Tensorflow graph:\n",
    "        summary_writer.add_graph(sess.graph)\n",
    "\n",
    "        # start timer for total training time:\n",
    "        start_total_time = time.time()\n",
    "        # set timer to measure the displayed training steps:\n",
    "        start_time = start_total_time\n",
    "\n",
    "        ########################################################################\n",
    "        \n",
    "        # Training:\n",
    "        # train loop\n",
    "        # train until all data is processed (queue empty),\n",
    "        # number of iterations depends on number of data, number of epochs and \n",
    "        # batch size:\n",
    "        iter_start = data_train.iter_left\n",
    "        logger.info('For training it takes {}\\n'.format(iter_start) +\n",
    "                    '    (= # data / batch_size * epochs) iterations to loop ' +\n",
    "                    'through {} samples of\\n  '.format(data_train.df.shape[0]) +\n",
    "                    '  training data over {} '.format(data_train.num_epochs) +\n",
    "                    'epochs summarized in batches of size ' + \n",
    "                    '{}.\\n'.format(data_train.batch_size) +\n",
    "                    '    So, there are # data / batch_size = ' +\n",
    "                    '{}'.format(int(data_train.df.shape[0]/data_train.batch_size))+\n",
    "                    ' iterations per epoch.')\n",
    "\n",
    "        while data_train.iter_left >= 0:\n",
    "            try:\n",
    "                # take a (mini) batch of the training data:\n",
    "                deq_train = data_train.dequeue()\n",
    "                img_b, alb_b, shad_b, inv_b = iq.next_batch(deq=deq_train, \n",
    "                                                            output_shape=IMAGE_SHAPE,\n",
    "                                                            is_scale=True,\n",
    "                                                            is_flip=True, \n",
    "                                                            is_rotated=True,\n",
    "                                                            norm=True)\n",
    "                # run training/optimization step:\n",
    "                # Run one step of the model.  The return values are the \n",
    "                # activations from the `train_op` (which is discarded) and the \n",
    "                # `loss` Op.  To inspect the values of your Ops or variables, \n",
    "                # you may include them in the list passed to sess.run() and the \n",
    "                # value tensors will be returned in the tuple from the call.\n",
    "                feed_dict_tr = {x: img_b,\n",
    "                                y_albedo_label: alb_b,\n",
    "                                y_shading_label: shad_b,\n",
    "                                invalid_px_mask: inv_b,\n",
    "                                training: True}\n",
    "                sess.run(opt_step, feed_dict=feed_dict_tr)\n",
    "\n",
    "                ################################################################\n",
    "\n",
    "                # report training set accuracy every DISPLAY_STEP-th step:\n",
    "                if (data_train.num_iter) % DISPLAY_STEP == 0:\n",
    "                    # console output:\n",
    "                    train_loss_dict = {}\n",
    "                    for key, val in loss_dict.items():\n",
    "                        train_loss_dict[key] = sess.run(val, \n",
    "                                                        feed_dict=feed_dict_tr)\n",
    "\n",
    "                    dur_time = time.time() - start_time\n",
    "                    dur_time = ghelp.get_time_format(time_in_sec=dur_time)\n",
    "                    dur_time = ghelp.time_tuple_to_str(time_tuple=dur_time)\n",
    "                    logger.info('step {}: '.format(data_train.num_iter) +\n",
    "                                'training ({}) loss '.format(loss_opt) +\n",
    "                                '{:.4f}'.format(train_loss_dict[loss_opt]) + \n",
    "                                '\\n    (' +\n",
    "                                ', '.join(['{}: {:.4f}'.format(it[0], it[1]) \\\n",
    "                                           for it in train_loss_dict.items() \\\n",
    "                                           if it[0]!=loss_opt]) +\n",
    "                                ' , ET: {}).'.format(dur_time))\n",
    "                    # reset timer to measure the displayed training steps:\n",
    "                    start_time = time.time()\n",
    "\n",
    "                ################################################################\n",
    "\n",
    "                # Validation:\n",
    "                # display validation set accuracy and loss after each completed \n",
    "                # epoch (1 epoch ^= data_train.df.shape[0]/data_train.batch_size\n",
    "                # training steps => steps per epoch)\n",
    "                val_epoch = int(data_train.df.shape[0] / data_train.batch_size)\n",
    "                \n",
    "                if data_train.num_iter % val_epoch == 0:\n",
    "                    # After each training epoch we will use the complete \n",
    "                    # validation data set to calculate the error/accuracy on the\n",
    "                    # validation set:\n",
    "                    # loop through one validation data set epoch:\n",
    "                    # initialize dictionary which contains all validation \n",
    "                    # losses:\n",
    "                    valid_loss_dict = dict.fromkeys(loss_dict, 0)\n",
    "                    valid_steps_per_epoch = int(data_valid.df.shape[0] / \n",
    "                                                data_valid.batch_size)\n",
    "                    for j in range(valid_steps_per_epoch):\n",
    "                        # DISCLAIMER: we do not run the opt_step here (on \n",
    "                        # the validation data set) because we do not want to \n",
    "                        # train our network on the validation set. Important \n",
    "                        # for batch normalization and dropout \n",
    "                        # (training -> False).\n",
    "                        # get validation data set (mini) batch:\n",
    "                        lst = iq.next_batch(deq=data_valid.dequeue(), \n",
    "                                            output_shape=IMAGE_SHAPE,\n",
    "                                            is_scale=False,\n",
    "                                            is_flip=False,\n",
    "                                            is_rotated=False,\n",
    "                                            norm=True)\n",
    "                        img_b_val, alb_b_val, shad_b_val, inv_b_val = lst\n",
    "\n",
    "                        # calculate the mean loss of this validation batch and \n",
    "                        # sum it with the previous mean batch losses:\n",
    "                        fd_val = {x: img_b_val,\n",
    "                                         y_albedo_label: alb_b_val,\n",
    "                                         y_shading_label: shad_b_val,\n",
    "                                         invalid_px_mask: inv_b_val,\n",
    "                                         training: False}\n",
    "\n",
    "                        for key, val in loss_dict.items():\n",
    "                            # divide each loss loss by the iteration steps \n",
    "                            # (steps_per_epoch) to get the mean val loss:\n",
    "                            mean_val = val / valid_steps_per_epoch\n",
    "                            valid_loss_dict[key] += sess.run(mean_val, \n",
    "                                                             feed_dict=fd_val)\n",
    "                    # adding a mean loss summary op (for tensorboard):\n",
    "                    feed_dict_vl = {valid_dict[key]: valid_loss_dict[key] for \\\n",
    "                                    key in valid_dict.keys()}\n",
    "                    val_loss_sums = sess.run(valid_sums_merged,\n",
    "                                             feed_dict=feed_dict_vl)\n",
    "                    summary_writer.add_summary(summary=val_loss_sums, \n",
    "                                               global_step=data_train.num_iter)\n",
    "\n",
    "                    dur_time = time.time() - start_time\n",
    "                    dur_time = ghelp.get_time_format(time_in_sec=dur_time)\n",
    "                    dur_time = ghelp.time_tuple_to_str(time_tuple=dur_time)\n",
    "                    logger.info('step {} '.format(data_train.num_iter) +\n",
    "                                '(epoch ' + \n",
    "                                '{}):'.format(data_train.completed_epochs + 1) +\n",
    "                                ' mean validation losses:\\n    ' +\n",
    "                                ', '.join(['{}: {:.4f}'.format(it[0], it[1]) \\\n",
    "                                           for it in valid_loss_dict.items()]) +\n",
    "                                ' (ET: {}).'.format(dur_time))\n",
    "                    # reset timer to measure the displayed training steps:\n",
    "                    start_time = time.time()\n",
    "\n",
    "                ################################################################\n",
    "\n",
    "                if data_train.num_iter % SAVE_STEP == 0:\n",
    "                    # save checkpoint files to disk:\n",
    "                    save_path = saver.save(sess, LOGS_PATH + 'tfmodel',\n",
    "                                           global_step=data_train.num_iter)\n",
    "                    s = sess.run(merge_train_summaries, feed_dict=feed_dict_tr)\n",
    "                    # adds a Summary protocol buffer to the event file \n",
    "                    # (global_step: Number. Optional global step value to record\n",
    "                    # with the summary. Each stepp i is assigned to the \n",
    "                    # corresponding summary parameter.)\n",
    "                    summary_writer.add_summary(summary=s, \n",
    "                                               global_step=data_train.num_iter)\n",
    "                    dur_time = time.time() - start_time\n",
    "                    dur_time = ghelp.get_time_format(time_in_sec=dur_time)\n",
    "                    dur_time = ghelp.time_tuple_to_str(time_tuple=dur_time)\n",
    "                    logger.info('Saved data (step ' + \n",
    "                                '{}):\\n'.format(data_train.num_iter) +\n",
    "                                '    Checkpoint file written to: ' + \n",
    "                                '{} '.format(save_path) +\n",
    "                                '(ET: {}).'.format(dur_time))\n",
    "                    # reset timer to measure the displayed training steps:\n",
    "                    start_time = time.time()\n",
    "\n",
    "            # end while loop when there are no elements left to dequeue:\n",
    "            except IndexError:\n",
    "                end_total_time = time.time() - start_total_time\n",
    "                end_total_time = ghelp.get_time_format(end_total_time)\n",
    "                end_total_time = ghelp.time_tuple_to_str(time_tuple=end_total_time)\n",
    "                logger.info('Training done... total training time: ' + \n",
    "                            '{}.'.format(end_total_time))\n",
    "                break\n",
    "\n",
    "    logger.info('Finished training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If not available download vgg16 ckpt files to ./models/slim/checkpoints\n",
      "Data has apparently already been downloaded and unpacked.\n"
     ]
    }
   ],
   "source": [
    "m_height = 13  # multiplicate of image height size -> network is designed so \n",
    "    # that it can take images with shape of multiples of m\n",
    "m_width = m_height  # multiplicate of image width size -> network \n",
    "    # is designed so that it can take images with shape of multiples of m\n",
    "\n",
    "\n",
    "# # narihira2015:\n",
    "# nodes_name_dict = {'input': 'input:0',\n",
    "#                    'output_albedo': 'deconv_s2out_albedo/BiasAdd:0',\n",
    "#                    'output_shading': 'deconv_s2out_shading/BiasAdd:0'}\n",
    "\n",
    "# params = {'LOGS_PATH': 'logs/narihira2015/test',  # path to summary files\n",
    "#           'DATA_DIR': '/usr/udo/data/',\n",
    "#           'path_inference_graph': 'models/narihira2015/tfmodel_inference.meta',\n",
    "#           'path_restore_model': None, # e.g. 'logs/2/tfmodel-5' or None\n",
    "#           'restore_scope': None,\n",
    "#           'IMAGE_SHAPE': [32 * m_height, 32 * m_width, 3],\n",
    "#           'INITIAL_LEARNING_RATE': 5e-4,  # hyper param\n",
    "#           'loss_opt': 'berhu',\n",
    "#           'BATCH_SIZE': 16,  # hyper param\n",
    "#           'NUM_EPOCHS': 50,  # hyper param \n",
    "#           'DISPLAY_STEP': 20,\n",
    "#           'SAVE_STEP': 100,\n",
    "#           'nodes_name_dict': nodes_name_dict,\n",
    "#           'plot_inference_graph': False,\n",
    "#           'is_sample_size': False\n",
    "#          }\n",
    "\n",
    "\n",
    "# slim_vgg16_narihira2015:\n",
    "nodes_name_dict = {'input': 'input:0',\n",
    "                   'output_albedo': 'scale2/deconv6_s2_albedo/BiasAdd:0',\n",
    "                   'output_shading': 'scale2/deconv6_s2_shading/BiasAdd:0'}\n",
    "# download checkpoint files:\n",
    "url = \"http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz\"\n",
    "checkpoints_dir = './models/slim/checkpoints'\n",
    "print('If not available download vgg16 ckpt files to ' + checkpoints_dir)\n",
    "download.maybe_download_and_extract(url=url, \n",
    "                                    download_dir=checkpoints_dir,\n",
    "                                    print_download_progress=True)\n",
    "params = {'LOGS_PATH': 'logs/slim_vgg16_narihira2015/test/',  # path to summary files\n",
    "          'DATA_DIR': '/usr/udo/data/',\n",
    "          'path_inference_graph': 'models/slim/graphs/vgg16_narihira2015/tfmodel_inference.meta',\n",
    "          'path_restore_model': './models/slim/checkpoints/vgg_16.ckpt', # e.g. 'logs/2/tfmodel-5' or None\n",
    "          'restore_scope': 'vgg_16',\n",
    "          'IMAGE_SHAPE': [32 * m_height, 32 * m_width, 3],\n",
    "          'INITIAL_LEARNING_RATE': 5e-4,  # hyper param\n",
    "          'loss_opt': 'berhu',\n",
    "          'BATCH_SIZE': 16,  # hyper param\n",
    "          'NUM_EPOCHS': 50,  # hyper param \n",
    "          'DISPLAY_STEP': 20,\n",
    "          'SAVE_STEP': 100,\n",
    "          'nodes_name_dict': nodes_name_dict,\n",
    "          'plot_inference_graph': False,\n",
    "          'is_sample_size': False\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-11 22:12:14 DEBUG]: Python version: \n",
      "    3.6.0 (default, Dec 24 2016, 13:33:34) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)]\n",
      "    Tensorflow version: 1.3.0\n",
      "[2017-09-11 22:12:14 INFO]: Training on images of shape: [416, 416, 3]\n",
      "[2017-09-11 22:12:14 INFO]: Initial learning rate: 0.0005\n",
      "[2017-09-11 22:12:14 INFO]: Loss function used for optimization: berhu\n",
      "[2017-09-11 22:12:14 INFO]: Batch size: 16\n",
      "[2017-09-11 22:12:14 INFO]: # epochs: 50\n",
      "[2017-09-11 22:12:14 INFO]: Report training set loss every 20iteration.\n",
      "[2017-09-11 22:12:14 INFO]: Write summary and checkpoints to file every 100-th iteration.\n",
      "[2017-09-11 22:12:15 DEBUG]: Restored inference graph from\n",
      "    models/slim/graphs/vgg16_narihira2015/tfmodel_inference.meta\n",
      "[2017-09-11 22:12:15 INFO]: # of parameters that can be restored: 26.\n",
      "[2017-09-11 22:12:16 DEBUG]: Defined losses.\n",
      "[2017-09-11 22:12:18 DEBUG]: Defined optimization method.\n",
      "[2017-09-11 22:12:18 DEBUG]: Defined validation losses.\n",
      "[2017-09-11 22:12:18 DEBUG]: Finished building training graph.\n",
      "[2017-09-11 22:12:18 INFO]: Total parameters of network: 49005222\n",
      "[2017-09-11 22:12:18 DEBUG]: Imported training data (#: 690) from\n",
      "    data/data_sintel_shading_train.csv\n",
      "[2017-09-11 22:12:18 DEBUG]: Imported validation data (#: 135) from\n",
      "    data/data_sintel_shading_valid.csv\n",
      "[2017-09-11 22:12:18 INFO]: Start training:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/slim/checkpoints/vgg_16.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./models/slim/checkpoints/vgg_16.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-11 22:12:25 INFO]: For training it takes 2150\n",
      "    (= # data / batch_size * epochs) iterations to loop through 690 samples of\n",
      "    training data over 50 epochs summarized in batches of size 16.\n",
      "    So, there are # data / batch_size = 43 iterations per epoch.\n"
     ]
    }
   ],
   "source": [
    "train_network(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-25T14:42:31.406731Z",
     "start_time": "2017-08-25T14:42:31.382090Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !tensorboard --logdir /logs/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0+"
  },
  "notify_time": "5",
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
