{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Import inference graph and extend it to a training graph:\n",
    "\n",
    "To extend the graph, we have to add:\n",
    "    - input data structure\n",
    "    - loss function\n",
    "    - optimization op\n",
    "    \n",
    "To build the graph it is necessary to know the node names of all relevant \n",
    "layers, placeholders etc. for restoring the model later.\n",
    "\n",
    "pay ATTENTION to:\n",
    "    the imported model must be in the same graph as the nodes which are \n",
    "    added later\n",
    "    -> load the model first to the default graph, then add further ops\n",
    "    \n",
    "also test to input input-images of different sizes (multiples of 32px). \n",
    "It might not work because input placeholder is defined fix.\n",
    "perhaps input node should not be saved inside the model?!\n",
    "\n",
    "output of model has name (deconv_s2out_shading/BiasAdd:0 and \n",
    "deconv_s2out_albedo/BiasAdd:0). \n",
    "simpler names?!\n",
    "\n",
    "\n",
    "To plot all graphs directly in this notebook, run jupyter form terminal like \n",
    "this:\n",
    "    jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-02T21:55:17.719495Z",
     "start_time": "2017-08-02T21:55:14.593366Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: \n",
      "3.6.0 (default, Dec 24 2016, 13:33:34) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)]\n",
      "Tensorflow version: \n",
      "1.2.1\n"
     ]
    }
   ],
   "source": [
    "import os   \n",
    "import sys\n",
    "sys.path.append('./util')\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import input_queues as iq\n",
    "import cnn_model\n",
    "import plot_helpers as plt_help\n",
    "import general_helpers as ghelp\n",
    "import cnn_helpers as cnnhelp\n",
    "\n",
    "print('Python version: \\n' + sys.version)\n",
    "print('Tensorflow version: \\n' + tf.__version__)\n",
    "\n",
    "# make only 'gpu:0' visible, so that only one gpu is used not both, see also\n",
    "# https://github.com/tensorflow/tensorflow/issues/5066\n",
    "# https://github.com/tensorflow/tensorflow/issues/3644#issuecomment-237631171\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "# data path constants:\n",
    "# DATA_DIR = '../data/mnist/'\n",
    "DATA_DIR = 'data/'\n",
    "# DATA_DIR = '/usr/udo/data/'\n",
    "PREDICT_PATH = ''\n",
    "path_inference_graph = ['logs/inference_graphs/narihira2015/' +\n",
    "                        'tfmodel_inference.meta']\n",
    "# path_inference_graph = ['/Users/udodehm/Downloads/camp_depth_irolaina/' + '\n",
    "#                         'ResNet_pretrained/ResNet-L50.meta']\n",
    "# path_inference_graph = ['vgg16/vgg16.tfmodel']\n",
    "\n",
    "path_inference_graph = path_inference_graph[0]\n",
    "path_restore_model = None #'logs/2/tfmodel-5'\n",
    "LOGS_PATH = 'logs/1/'  # path to summary files\n",
    "\n",
    "# hyper-parameters:\n",
    "m_height = 13  # multiplicate of image height size -> network is designed so \n",
    "    # that it can take images with shape of multiples of m\n",
    "m_width = m_height  # multiplicate of image width size -> network \n",
    "    # is designed so that it can take images with shape of multiples of m\n",
    "IMAGE_SHAPE = [32 * m_height, 32 * m_width, 3]  # complete image size \n",
    "    # [436, 1024, 3] # Narihira2015 use [M*32=13*32=416, 416, 3]\n",
    "INITIAL_LEARNING_RATE = 1e-5\n",
    "# probability that a neuron's output is kept during dropout (only during \n",
    "# training!!!, testing/validation -> 1.0):\n",
    "# DROPOUT_RATE = 0.5\n",
    "BATCH_SIZE = 8  # nr of data which is put through the network before updating \n",
    "    # it, as default use: 32. \n",
    "# BATCH_SIZE determines how many data samples are loaded in the memory (be \n",
    "# careful with memory space)\n",
    "NUM_EPOCHS = 2  # nr of times the training process loops through the \n",
    "    # complete training data set (how often is the tr set 'seen')\n",
    "    # if you have 1000 training examples, and your batch size is 500, then it\n",
    "    # will take 2 iterations to complete 1 epoch.\n",
    "\n",
    "DISPLAY_STEP = 2  # every DIPLAY_STEP'th training iteration information is \n",
    "    # printed (default: 100)\n",
    "SUMMARY_STEP = 2  # every SUMMARY_STEP'th training iteration a summary file is \n",
    "    # written to LOGS_PATH\n",
    "DEVICE = '/cpu:0'  # device on which the variable is saved/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-02T21:55:17.731379Z",
     "start_time": "2017-08-02T21:55:17.722744Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# graph = tf.Graph()\n",
    "  \n",
    "# # Set the new graph as the default.\n",
    "# with graph.as_default():\n",
    "  \n",
    "#     # Open the graph-def file for binary reading.\n",
    "#     path = path_inference_graph\n",
    "#     with tf.gfile.FastGFile(path, 'rb') as file:\n",
    "#         # The graph-def is a saved copy of a TensorFlow graph.\n",
    "#         # First we need to create an empty graph-def.\n",
    "#         graph_def = tf.GraphDef()\n",
    "\n",
    "#         # Then we load the proto-buf file into the graph-def.\n",
    "#         graph_def.ParseFromString(file.read())\n",
    "\n",
    "#         # Finally we import the graph-def to the default TensorFlow graph.\n",
    "#         tf.import_graph_def(graph_def, name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-02T21:55:21.441562Z",
     "start_time": "2017-08-02T21:55:17.734634Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# load meta graph (inference graph)\n",
    "# how to work with restored models:\n",
    "# https://www.tensorflow.org/programmers_guide/meta_graph\n",
    "# http://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/\n",
    "saver_restore = tf.train.import_meta_graph(path_inference_graph, \n",
    "                                           clear_devices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-02T21:55:21.448002Z",
     "start_time": "2017-08-02T21:55:21.443847Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# save default graph in variable:\n",
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-02T21:55:21.455848Z",
     "start_time": "2017-08-02T21:55:21.451606Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # plot imported inference graph:\n",
    "# plt_help.show_graph(graph.as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-02T21:55:21.504717Z",
     "start_time": "2017-08-02T21:55:21.460428Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# lets get the input\n",
    "x = graph.get_tensor_by_name(name='input:0')\n",
    "\n",
    "# setup target output classes (ground truth):\n",
    "y_albedo_label = tf.placeholder(dtype=tf.float32, \n",
    "                                shape=[None] + IMAGE_SHAPE, \n",
    "                                name='out_albedo')\n",
    "y_shading_label = tf.placeholder(dtype=tf.float32, \n",
    "                                 shape=[None] + IMAGE_SHAPE, \n",
    "                                 name='out_shading')\n",
    "\n",
    "# bool variable that indicates if we are in training mode (training=True) or \n",
    "# valid/test mode (training=False) this indicator is important if dropout or/and\n",
    "# batch normalization is used.\n",
    "training = graph.get_tensor_by_name(name='is_training:0')\n",
    "# get graph output nodes:\n",
    "y_albedo_pred = graph.get_tensor_by_name(name='deconv_s2out_albedo/BiasAdd:0')\n",
    "y_shading_pred = graph.get_tensor_by_name(name='deconv_s2out_shading/BiasAdd:0')\n",
    "# y_albedo_pred = tf.clip_by_value(t=y_albedo, clip_value_min=0, \n",
    "#                                  clip_value_max=1, name='0_1_clipping_albedo')\n",
    "# y_shading_pred = tf.clip_by_value(t=y_shading, clip_value_min=0,\n",
    "#                                   clip_value_max=1, \n",
    "#                                   name='0_1_clipping_shading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-02T21:55:27.298795Z",
     "start_time": "2017-08-02T21:55:21.508187Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "loss = cnnhelp.loss_fct(label_albedo=y_albedo_label, \n",
    "                        label_shading=y_shading_label, \n",
    "                        prediction_albedo=y_albedo_pred, \n",
    "                        prediction_shading=y_shading_pred, \n",
    "                        lambda_=0.5)\n",
    "\n",
    "# Use an AdamOptimizer to train the network:\n",
    "with tf.name_scope('optimization'):\n",
    "    opt_step = tf.train.AdamOptimizer(INITIAL_LEARNING_RATE).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-02T21:55:28.365742Z",
     "start_time": "2017-08-02T21:55:27.301265Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# to get every summary defined above we merge them to get one target:\n",
    "merge_train_summaries = tf.summary.merge_all()\n",
    "\n",
    "# define a FileWriter op which writes summaries defined above to disk:\n",
    "summary_writer = tf.summary.FileWriter(LOGS_PATH)\n",
    "\n",
    "# Create a saver for writing training checkpoints.\n",
    "saver = tf.train.Saver(max_to_keep=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-02T21:55:28.384421Z",
     "start_time": "2017-08-02T21:55:28.368453Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# introduce some validation set specific summaries\n",
    "# These summaries need to be defined blow the function \n",
    "# merge_train_summaries = tf.summary.merge_all()\n",
    "# because the validation set summaries are added to the summary writer at \n",
    "# different times. If they had been summarized with the training summaries they \n",
    "# would have to be defined at times where merge_train_summaries are added to the\n",
    "# summary writer\n",
    "with tf.name_scope('loss/'):\n",
    "    valid_loss = tf.placeholder(dtype=tf.float32)\n",
    "    valid_loss_summary = tf.summary.scalar(name='validation_loss', \n",
    "                                           tensor=valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-02T21:55:14.623Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # plot complete graph:\n",
    "# plt_help.show_graph(graph.as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-02T21:55:14.628Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: For training it takes 12 (= # data / batch_size * epochs) iterations to loop through 50 samples of training data over 2 epochs summarized in batches of size 8.\n",
      "So, there are # data / batch_size = 6 iterations per epoch.\n",
      "\n",
      "iteration 2: training loss 0.29 (ET: 01:43 min).\n"
     ]
    }
   ],
   "source": [
    "# import data:\n",
    "# import training data:\n",
    "file = 'sample_data_sintel_shading_train.csv'\n",
    "df_train = pd.read_csv(DATA_DIR + file, sep=',', header=None,\n",
    "                       names=['img', 'alb', 'shad'])\n",
    "# compolete image paths:\n",
    "df_train = DATA_DIR + df_train\n",
    "# instantiate a data queue for feeding data in (mini) batches to cnn:\n",
    "data_train = iq.DataQueue(df=df_train, batch_size=BATCH_SIZE,\n",
    "                          num_epochs=NUM_EPOCHS)\n",
    "\n",
    "# import validation data set: \n",
    "# why not using the whole validation set for validation at once? \n",
    "# - limited memory space.\n",
    "#  -> After each training epoch we will use the complete validation dataset\n",
    "#     to calculate the error/accuracy on the validation set\n",
    "file = 'sample_data_sintel_shading_valid.csv'\n",
    "df_valid = pd.read_csv(DATA_DIR + file, sep=',', header=None,\n",
    "                       names=['img', 'alb', 'shad'])\n",
    "# compolete image paths:\n",
    "df_valid = DATA_DIR + df_valid\n",
    "# instantiate a data queue for feeding data in (mini) batches to cnn:\n",
    "data_valid = iq.DataQueue(df=df_valid, batch_size=BATCH_SIZE,\n",
    "                          num_epochs=NUM_EPOCHS)\n",
    "\n",
    "# testing data set: \n",
    "file = 'sample_data_sintel_shading_test.csv'\n",
    "df_test = pd.read_csv(DATA_DIR + file, sep=',', header=None,\n",
    "                      names=['img', 'alb', 'shad'])\n",
    "# compolete image paths:\n",
    "df_test = DATA_DIR + df_test\n",
    "# instantiate a data queue for feeding data in (mini) batches to cnn:\n",
    "data_test = iq.DataQueue(df=df_test, batch_size=BATCH_SIZE,\n",
    "                         num_epochs=1)\n",
    "\n",
    "################################################################################\n",
    "\n",
    "# Initialization:\n",
    "# Op that initializes global variables in the graph:\n",
    "init_global = tf.global_variables_initializer()\n",
    "# Op that initializes local variables in the graph:\n",
    "init_local = tf.local_variables_initializer()\n",
    "\n",
    "# config = tf.ConfigProto(allow_soft_placement = True,\n",
    "#                         intra_op_parallelism_threads=3,\n",
    "#                         log_device_placement=False)\n",
    "config = tf.ConfigProto(device_count = {'GPU': 1},\n",
    "                        intra_op_parallelism_threads=3)\n",
    "with tf.Session(config=config) as sess: \n",
    "# with tf.Session() as sess:\n",
    "    ############################################################################\n",
    "    # initialize all variables:\n",
    "    sess.run([init_global, init_local])\n",
    "    \n",
    "    if path_restore_model:\n",
    "        # restore saved model parameters (weights, biases, etc):\n",
    "        saver_restore.restore(sess, path_restore_model)\n",
    "    \n",
    "    # Adds a Graph to the event file.\n",
    "    # create summary that give output (TensorFlow op that output protocol \n",
    "    # buffers containing 'summarized' data) of the built Tensorflow graph:\n",
    "    summary_writer.add_graph(sess.graph)\n",
    "    \n",
    "    # start timer for total training time:\n",
    "    start_total_time = time.time()\n",
    "    # set timer to measure the displayed training steps:\n",
    "    start_time = start_total_time\n",
    "    \n",
    "    ############################################################################\n",
    "    # Training:\n",
    "    # train loop\n",
    "    #     train for until all data is used\n",
    "    #     number of iterations depends on number of data, number of epochs and \n",
    "    #     batch size:\n",
    "    iter_start = data_train.iter_left\n",
    "    print('INFO: For training it takes {} '.format(iter_start) +\n",
    "          '(= # data / batch_size * epochs) iterations to loop through ' +\n",
    "          '{} samples of training data over '.format(data_train.df.shape[0]) +\n",
    "          '{} epochs summarized in batches '.format(data_train.num_epochs) +\n",
    "          'of size {}.\\n'.format(data_train.batch_size) +\n",
    "          'So, there are # data / batch_size = ' +\n",
    "          '{} '.format(int(data_train.df.shape[0] / data_train.batch_size)) + \n",
    "          'iterations per epoch.\\n')\n",
    "    \n",
    "    while data_train.iter_left >= 0:\n",
    "        try:\n",
    "            # take a (mini) batch of the training data:\n",
    "            deq_train = data_train.dequeue()\n",
    "            img_batch, alb_batch, shad_batch = iq.next_batch(deq=deq_train, \n",
    "                                                             shape=IMAGE_SHAPE, \n",
    "                                                             is_flip=True, \n",
    "                                                             is_rotated=False,\n",
    "                                                             norm=True)\n",
    "            # run training/optimization step:\n",
    "            # Run one step of the model.  The return values are the activations\n",
    "            # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "            # inspect the values of your Ops or variables, you may include them\n",
    "            # in the list passed to sess.run() and the value tensors will be\n",
    "            # returned in the tuple from the call.\n",
    "\n",
    "            feed_dict = {x: img_batch,\n",
    "                         y_albedo_label: alb_batch,\n",
    "                         y_shading_label: shad_batch,\n",
    "                         training: True}\n",
    "            sess.run(opt_step, feed_dict=feed_dict)\n",
    "\n",
    "            # report training set accuracy every DISPLAY_STEP-th step:\n",
    "            if (data_train.num_iter) % DISPLAY_STEP == 0:\n",
    "                # console output:\n",
    "                feed_dict = {x: img_batch,\n",
    "                             y_albedo_label: alb_batch,\n",
    "                             y_shading_label: shad_batch,\n",
    "                             training: False}\n",
    "                train_loss = sess.run(loss, \n",
    "                                      feed_dict=feed_dict)\n",
    "                duration_time = time.time() - start_time\n",
    "                duration_time = ghelp.get_time_format(time_in_sec=duration_time)\n",
    "                duration_time = ghelp.time_tuple_to_str(time_tuple=duration_time)\n",
    "                print('iteration {}: training '.format(data_train.num_iter) + \n",
    "                      'loss {tr_loss:.2f} (ET: '.format(tr_loss=train_loss) +\n",
    "                      '{}).'.format(duration_time))\n",
    "                # reset timer to measure the displayed training steps:\n",
    "                start_time = time.time()\n",
    "\n",
    "        ########################################################################\n",
    "            # Validation:\n",
    "            # display validation set accuracy and loss after each completed \n",
    "            # epoch (1 epoch ^= data_train.df.shape[0] / data_train.batch_size \n",
    "            # training steps => steps per epoch)\n",
    "            val_epoch = int(data_train.df.shape[0] / data_train.batch_size)\n",
    "            if data_train.num_iter % val_epoch == 0:\n",
    "                # save checkpoint files to disk:\n",
    "                save_path = saver.save(sess, LOGS_PATH + 'tfmodel',\n",
    "                                       global_step=data_train.num_iter)\n",
    "                print('Validation scores after epoch ' + \n",
    "                      '{} '.format(data_train.completed_epochs + 1) + \n",
    "                      '(step {}):\\n'.format(data_train.num_iter) +\n",
    "                      '    Model saved in file: {}.'.format(save_path))\n",
    "                # After each training epoch we will use the complete validation \n",
    "                # data set to calculate the error/accuracy on the validation \n",
    "                # set:\n",
    "                # loop through one validation data set epoch:\n",
    "                validation_loss = 0\n",
    "                valid_steps_per_epoch = int(data_valid.df.shape[0] / \n",
    "                                            data_valid.batch_size)\n",
    "                for j in range(valid_steps_per_epoch):\n",
    "                    # DISCLAIMER: we do not run the opt_step here (on \n",
    "                    # the validation data set) because we do not want to train\n",
    "                    # our network on the validation set. Important for batch \n",
    "                    # normalization and dropout (training -> False).\n",
    "                    # get validation data set (mini) batch:\n",
    "                    lst = iq.next_batch(deq=data_valid.dequeue(), \n",
    "                                        shape=IMAGE_SHAPE, \n",
    "                                        is_flip=True,\n",
    "                                        is_rotated=False,\n",
    "                                        norm=True)\n",
    "                    img_batch_val, alb_batch_val, shad_batch_val = lst\n",
    "                    \n",
    "                    # calculate the mean loss of this validation batch and sum \n",
    "                    # it with the previous mean batch losses:\n",
    "                    feed_dict = {x: img_batch_val,\n",
    "                                 y_albedo_label: alb_batch_val,\n",
    "                                 y_shading_label: shad_batch_val,\n",
    "                                 training: False}\n",
    "                    validation_loss += sess.run(loss, \n",
    "                                                feed_dict=feed_dict)\n",
    "\n",
    "                # adding a mean loss summary op (for tensorboard). \n",
    "                # we need to divide the accumulated loss from above by the \n",
    "                # iteration steps (steps_per_epoch):\n",
    "                feed_dict = {valid_loss: validation_loss / valid_steps_per_epoch}\n",
    "                validation_loss_total = sess.run(valid_loss_summary, \n",
    "                                                 feed_dict=feed_dict)\n",
    "                summary_writer.add_summary(summary=validation_loss_total, \n",
    "                                           global_step=data_train.num_iter)\n",
    "                duration_time = time.time() - start_time\n",
    "                duration_time = ghelp.get_time_format(time_in_sec=duration_time)\n",
    "                duration_time = ghelp.time_tuple_to_str(time_tuple=duration_time)\n",
    "                print('    Num validation data: ' +\n",
    "                      '{}, mean loss: '.format(data_valid.df.shape[0]) +\n",
    "                      '{ml:.2f}'.format(ml=validation_loss / valid_steps_per_epoch) +\n",
    "                      ' (ET: {}).'.format(duration_time))\n",
    "                # reset timer to measure the displayed training steps:\n",
    "                start_time = time.time()\n",
    "\n",
    "            if data_train.num_iter % SUMMARY_STEP == 0:\n",
    "                feed_dict = {x: img_batch,  \n",
    "                             y_albedo_label: alb_batch,\n",
    "                             y_shading_label: shad_batch, \n",
    "                             training: False}\n",
    "                s = sess.run(merge_train_summaries, feed_dict=feed_dict)\n",
    "                # adds a Summary protocol buffer to the event file \n",
    "                #     (global_step: Number. Optional global step value to record\n",
    "                #     with the summary. Each stepp i is assigned to the \n",
    "                #     corresponding summary parameter.)\n",
    "                summary_writer.add_summary(summary=s, \n",
    "                                           global_step=data_train.num_iter)\n",
    "        # end while loop when there are no elements left to dequeue:\n",
    "        except IndexError:\n",
    "            end_total_time = time.time() - start_total_time\n",
    "            end_total_time = ghelp.get_time_format(end_total_time)\n",
    "            end_total_time = ghelp.time_tuple_to_str(time_tuple=end_total_time)\n",
    "            print('\\nTraining done... total training time: ' + \n",
    "                  '{}.'.format(end_total_time))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-02T21:55:14.634Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# print all op and tensor names in default graph:\n",
    "# len([print(n.name) for n in graph.as_graph_def().node])\n",
    "# list all global variables:\n",
    "# tf.global_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-08-02T21:55:14.640Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !tensorboard --logdir ./logs/1"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "notify_time": "5",
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
