{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-31T14:05:02.277830Z",
     "start_time": "2017-08-31T14:05:00.568554Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import inference graph, extend it to a training graph and train network:\n",
    "\n",
    "To extend the graph, we have to add:\n",
    "    - input data structure\n",
    "    - loss function\n",
    "    - optimization op\n",
    "\n",
    "- outdated: To plot all graphs directly in this notebook, run jupyter form\n",
    "  terminal like this:\n",
    "      jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000000\n",
    "- continously tracking nvidia gpu: nvidia-smi -l 10\n",
    "- converting a jupyter notebook file to python script: \n",
    "      jupyter nbconvert --to script cnns.ipynb\n",
    "\"\"\"\n",
    "\n",
    "import os   \n",
    "import sys\n",
    "sys.path.append('./util')\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import input_queues as iq\n",
    "import cnn_model\n",
    "import plot_helpers as plt_help\n",
    "import general_helpers as ghelp\n",
    "import nn_helpers as nnhelp\n",
    "import cnn_helpers as chlp\n",
    "import download\n",
    "\n",
    "\n",
    "__author__ = \"Udo Dehm\"\n",
    "__copyright__ = \"Copyright 2017\"\n",
    "__credits__ = [\"Udo Dehm\"]\n",
    "__license__ = \"\"\n",
    "__version__ = \"0.1\"\n",
    "__maintainer__ = \"Udo Dehm\"\n",
    "__email__ = \"udo.dehm@mailbox.org\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "__all__ = ['train_network']  \n",
    "\n",
    "\n",
    "# make only 'gpu:0' visible, so that only one gpu is used not both, see also\n",
    "# https://github.com/tensorflow/tensorflow/issues/5066\n",
    "# https://github.com/tensorflow/tensorflow/issues/3644#issuecomment-237631171\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "def train_network(log_dir, data_dir, path_inference_graph, path_restore_model,\n",
    "                  restore_scope, image_shape, initial_learning_rate, loss_opt, \n",
    "                  batch_size, num_epochs, display_step, save_step, \n",
    "                  nodes_name_dict, norm=False, plot_inference_graph=False,\n",
    "                  is_sample_size=False):\n",
    "    \"\"\"\n",
    "    :param log_dir: path to directory for saving summary/log files\n",
    "    :type log_dir: str\n",
    "    :param data_dir: path to directory with training and validation data (in \n",
    "        this directory the training and validation csv files have to be\n",
    "        located.)\n",
    "    :type data_dir: str\n",
    "    :param path_inference_graph: path to inference graph (graph without \n",
    "        'training' ops)\n",
    "    :type path_inference_graph: str\n",
    "    :param path_restore_model: path to model parameters (checkpoint files\n",
    "        e.g. 'logs/2/tfmodel-5' or None)\n",
    "    :type path_restore_model: str\n",
    "    :param restore_scope: a scope name which has to be defined to find \n",
    "        parameters to restore (e.g. 'vgg_16'). Usually this is needed to\n",
    "        find a pre-trained (tf slim) model within another model.\n",
    "    :type restore_scope: str\n",
    "    :param image_shape: Shape of images that should be used for training \n",
    "        (shape of cnn input tensor)\n",
    "    :type image_shape: list with len(image_shape)==3\n",
    "    :param initial_learning_rate: hyper-parameters for initial learning rate\n",
    "    :type initial_learning_rate: float\n",
    "    :param loss_opt: loss function used for optimization ('berhu_log', \n",
    "        'berhu', 'l2_log', 'l2', 'l2_inv_log', 'l2_inv', 'l2_avg_log', \n",
    "        'l2_avg')\n",
    "    :type loss_opt: str\n",
    "    :param batch_size: nr of data which is put through the network before \n",
    "        updating it, as default use: 16, 32 or 64. \n",
    "        batch_size determines how many data samples are loaded in the memory \n",
    "        (be careful with memory space)\n",
    "    :type batch_size: int\n",
    "    :param num_epochs: nr of times the training process loops through the \n",
    "        complete training data set (how often is the tr set 'seen')\n",
    "        if you have 1000 training examples, and your batch size is 500, then it\n",
    "        will take 2 iterations to complete 1 epoch.\n",
    "    :type num_epochs: int\n",
    "    :param display_step: every display_step'th training iteration information is\n",
    "        printed to stdout and file training.log in log_dir (default: 100)\n",
    "    :type display_step: int\n",
    "    :param save_step: every save_step'th training iteration a summary file is \n",
    "        written to log_dir and checkpoint files are saved\n",
    "    :type save_step: int\n",
    "    :param nodes_name_dict: dictionary that contains name of input, albedo and \n",
    "        shading output in form {'input': '', \n",
    "                                'output_albedo': '',\n",
    "                                'output_shading': ''}\n",
    "    :type nodes_name_dict: dict\n",
    "    :param norm: flag, if True image pixels are scaled to range [0, 1]\n",
    "        (default: False)\n",
    "    :type norm: boolean\n",
    "    :param plot_inference_graph: flag, True if inference graph should be \n",
    "        plotted (default: False).\n",
    "    :type plot_inference_graph: boolean\n",
    "    :param is_sample_size: flag, if True only a smaller sample size is used for \n",
    "        training and validation.\n",
    "    :type is_sample_size: boolean\n",
    "    \"\"\"\n",
    "    ############################################################################\n",
    "\n",
    "    # create logger (write to file and stdout):\n",
    "    logger = ghelp.create_logger(filename=log_dir + 'training.log')\n",
    "    logger.debug('Python version: \\n    ' + sys.version + \n",
    "                 '\\n    Tensorflow version: ' + tf.__version__)\n",
    "    logger.info('Training on images of shape: {}'.format(image_shape))\n",
    "    logger.info('Initial learning rate: {}'.format(initial_learning_rate))\n",
    "    logger.info('Loss function used for optimization: {}'.format(loss_opt))\n",
    "    logger.info('Batch size: {}'.format(batch_size))\n",
    "    logger.info('# epochs: {}'.format(num_epochs))\n",
    "    logger.info('Report training set loss every {}'.format(display_step) +\n",
    "                'iteration.')\n",
    "    logger.info('Write summary and checkpoints to file every ' +\n",
    "                '{}-th iteration.'.format(save_step))\n",
    "\n",
    "    # load meta graph (inference graph)\n",
    "    # how to work with restored models:\n",
    "    # https://www.tensorflow.org/programmers_guide/meta_graph\n",
    "    # http://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/\n",
    "    saver_restore = tf.train.import_meta_graph(path_inference_graph, \n",
    "                                               clear_devices=True)\n",
    "\n",
    "    logger.debug('Restored inference graph from\\n' +\n",
    "                 '    {}'.format(path_inference_graph))\n",
    "    variables_to_restore = slim.get_variables_to_restore(include=[restore_scope],\n",
    "                                                         exclude=None)\n",
    "    logger.info('# of parameters that can be restored: ' + \n",
    "                '{}.'.format(len(variables_to_restore)))\n",
    "\n",
    "    ############################################################################\n",
    "\n",
    "    # save default graph in variable:\n",
    "    graph = tf.get_default_graph()\n",
    "    if plot_inference_graph:\n",
    "        # plot imported inference graph:\n",
    "        plt_help.show_graph(graph.as_graph_def())\n",
    "\n",
    "    # lets get the input\n",
    "    x = graph.get_tensor_by_name(name=nodes_name_dict['input'])\n",
    "\n",
    "    # setup target output classes (ground truth):\n",
    "    y_albedo_label = tf.placeholder(dtype=tf.float32, \n",
    "                                    shape=[None] + image_shape, \n",
    "                                    name='out_albedo')\n",
    "    y_shading_label = tf.placeholder(dtype=tf.float32, \n",
    "                                     shape=[None] + image_shape, \n",
    "                                     name='out_shading')\n",
    "\n",
    "    # bool variable that indicates if we are in training mode (training=True) or\n",
    "    # valid/test mode (training=False) this indicator is important if dropout \n",
    "    # or/and batch normalization is used.\n",
    "    try:\n",
    "        # try importing training node (is needed for models that use batch \n",
    "        # normalization etc.)\n",
    "        training = graph.get_tensor_by_name(name='is_training:0')\n",
    "    except KeyError:\n",
    "        # elsewise just define a placeholder wich is used as dummy variable\n",
    "        # and won't be used later:\n",
    "        training = tf.placeholder(dtype=tf.bool, name='is_training')\n",
    "\n",
    "    invalid_px_mask = tf.placeholder(dtype=tf.float32, \n",
    "                                     shape=[None] + image_shape, \n",
    "                                     name='invalid_px_mask')\n",
    "\n",
    "    # get graph output nodes:\n",
    "    y_albedo_pred = graph.get_tensor_by_name(name=nodes_name_dict['output_albedo'])\n",
    "    y_shading_pred = graph.get_tensor_by_name(name=nodes_name_dict['output_shading'])\n",
    "    # y_albedo_pred = tf.clip_by_value(t=y_albedo, clip_value_min=0, \n",
    "    #                                  clip_value_max=1, \n",
    "    #                                  name='0_1_clipping_albedo')\n",
    "    # y_shading_pred = tf.clip_by_value(t=y_shading, clip_value_min=0,\n",
    "    #                                   clip_value_max=1, \n",
    "    #                                   name='0_1_clipping_shading')\n",
    "\n",
    "    ############################################################################\n",
    "    ############################################################################\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        valid_mask = chlp.get_valid_pixels(image=x, \n",
    "                                           invalid_mask=invalid_px_mask)\n",
    "        d = {'label_albedo': y_albedo_label,\n",
    "             'label_shading': y_shading_label,\n",
    "             'prediction_albedo': y_albedo_pred, \n",
    "             'prediction_shading': y_shading_pred,\n",
    "             'valid_mask': valid_mask}\n",
    "        \n",
    "        loss_dict = {'berhu_log': chlp.loss_fct(**d, **{'loss_type': 'berhu',\n",
    "                                                        'lambda_': None, \n",
    "                                                        'log': True}),\n",
    "                     'berhu': chlp.loss_fct(**d, **{'loss_type': 'berhu', \n",
    "                                                    'lambda_': None, \n",
    "                                                    'log': False}),\n",
    "                     'l2_log': chlp.loss_fct(**d, **{'loss_type': 'mse', \n",
    "                                                     'lambda_': 0, \n",
    "                                                     'log': True}),\n",
    "                     'l2': chlp.loss_fct(**d, **{'loss_type': 'mse', \n",
    "                                                 'lambda_': 0, \n",
    "                                                 'log': False}),\n",
    "                     'l2_inv_log': chlp.loss_fct(**d, **{'loss_type': 'mse', \n",
    "                                                         'lambda_': 1, \n",
    "                                                         'log': True}),\n",
    "                     'l2_inv': chlp.loss_fct(**d, **{'loss_type': 'mse',\n",
    "                                                     'lambda_': 1,\n",
    "                                                     'log': False}),\n",
    "                     'l2_avg_log': chlp.loss_fct(**d, **{'loss_type': 'mse', \n",
    "                                                         'lambda_': 0.5,\n",
    "                                                         'log': True}),\n",
    "                     'l2_avg': chlp.loss_fct(**d, **{'loss_type': 'mse',\n",
    "                                                     'lambda_': 0.5, \n",
    "                                                     'log': False})\n",
    "                    }\n",
    "        loss = loss_dict[loss_opt]\n",
    "    logger.debug('Defined losses.')\n",
    "\n",
    "    ############################################################################\n",
    "\n",
    "    # Use an AdamOptimizer to train the network:\n",
    "    with tf.name_scope('optimization'):\n",
    "        opt_step = tf.train.AdamOptimizer(initial_learning_rate).minimize(loss)\n",
    "    logger.debug('Defined optimization method.')\n",
    "\n",
    "    ###########################################################################\n",
    "\n",
    "    # to get every summary defined above we merge them to get one target:\n",
    "    merge_train_summaries = tf.summary.merge_all()\n",
    "    # define a FileWriter op which writes summaries defined above to disk:\n",
    "    summary_writer = tf.summary.FileWriter(log_dir)\n",
    "    # Create a saver for writing training checkpoints.\n",
    "    saver = tf.train.Saver(max_to_keep=num_epochs)\n",
    "\n",
    "    ############################################################################\n",
    "\n",
    "    # introduce some validation set specific summaries\n",
    "    # These summaries need to be defined blow the function \n",
    "    # merge_train_summaries = tf.summary.merge_all()\n",
    "    # because the validation set summaries are added to the summary writer at \n",
    "    # different times. If they had been summarized with the training summaries \n",
    "    # they would have to be defined at times where merge_train_summaries are \n",
    "    # added to the summary writer\n",
    "    with tf.name_scope('loss/valid/'):\n",
    "        valid_dict = {key: tf.placeholder(dtype=tf.float32, name=key) for \\\n",
    "                      key in loss_dict.keys()}\n",
    "        valid_summaries = [tf.summary.scalar(name=key, tensor=val) for \\\n",
    "                           key, val in valid_dict.items()]\n",
    "        valid_sums_merged = tf.summary.merge(inputs=valid_summaries,\n",
    "                                             collections=None, name=None)\n",
    "    logger.debug('Defined validation losses.')\n",
    "    logger.debug('Finished building training graph.')\n",
    "    logger.info('Total parameters of network: ' +\n",
    "                '{}'.format(nnhelp.network_params()))\n",
    "\n",
    "    ############################################################################\n",
    "    ############################################################################\n",
    "\n",
    "    # import data:\n",
    "    if is_sample_size:\n",
    "        sample = 'sample_'\n",
    "    else:\n",
    "        sample = ''\n",
    "    # import training data:\n",
    "    file = sample + 'data_sintel_shading_train.csv'\n",
    "    df_train = pd.read_csv(data_dir + file, sep=',', header=None,\n",
    "                           names=['img', 'alb', 'shad', 'invalid'])\n",
    "    # complete image paths:\n",
    "    df_train = data_dir + df_train\n",
    "\n",
    "    # # enable this line to train on only one image:\n",
    "    # df_train1 = df_train.loc[[0]]\n",
    "    # # replicate this row 100 times:\n",
    "    # df_train = pd.concat([df_train1]*100).reset_index(drop=True)\n",
    "\n",
    "    # instantiate a data queue for feeding data in (mini) batches to cnn:\n",
    "    data_train = iq.DataQueue(df=df_train, batch_size=batch_size,\n",
    "                              num_epochs=num_epochs)\n",
    "    logger.debug('Imported training data ' + \n",
    "                 '(#: {}) '.format(data_train.df.shape[0]) + \n",
    "                 'from\\n    {}'.format(data_dir + file))\n",
    "\n",
    "    # import validation data set: \n",
    "    # why not using the whole validation set for validation at once? \n",
    "    # - limited memory space.\n",
    "    #  -> After each training epoch we will use the complete validation dataset\n",
    "    #     to calculate the error/accuracy on the validation set\n",
    "    file = sample + 'data_sintel_shading_valid.csv'\n",
    "    df_valid = pd.read_csv(data_dir + file, sep=',', header=None,\n",
    "                           names=['img', 'alb', 'shad', 'invalid'])\n",
    "    # complete image paths:\n",
    "    df_valid = data_dir + df_valid\n",
    "    # instantiate a data queue for feeding data in (mini) batches to cnn:\n",
    "    data_valid = iq.DataQueue(df=df_valid, batch_size=batch_size,\n",
    "                              num_epochs=num_epochs)\n",
    "    logger.debug('Imported validation data ' +\n",
    "                 '(#: {}) '.format(data_valid.df.shape[0]) + \n",
    "                 'from\\n    {}'.format(data_dir + file))\n",
    "\n",
    "    ############################################################################\n",
    "    ############################################################################\n",
    "\n",
    "    logger.info('Start training:')\n",
    "    # Initialization:\n",
    "    # Op that initializes global variables in the graph:\n",
    "    init_global = tf.global_variables_initializer()\n",
    "    # Op that initializes local variables in the graph:\n",
    "    init_local = tf.local_variables_initializer()\n",
    "\n",
    "    config = tf.ConfigProto(device_count = {'GPU': 1},\n",
    "                            intra_op_parallelism_threads=3\n",
    "    #                        allow_soft_placement = True,\n",
    "    #                        intra_op_parallelism_threads=3,\n",
    "    #                        log_device_placement=False\n",
    "                           )\n",
    "    with tf.Session(config=config) as sess: \n",
    "        ########################################################################\n",
    "        # initialize all variables:\n",
    "        sess.run([init_global, init_local])\n",
    "\n",
    "        if path_restore_model:\n",
    "            try:\n",
    "                # restore saved model parameters (weights, biases, etc):\n",
    "                saver_restore.restore(sess, path_restore_model)\n",
    "                logger.info('Restoring parameters from ' +\n",
    "                            '{}'.format(path_restore_model))\n",
    "            except (tf.errors.InvalidArgumentError, tf.errors.NotFoundError):\n",
    "                # in the worst case parameters are loaded twice.\n",
    "                # restore the parameters:\n",
    "                init_fn = slim.assign_from_checkpoint_fn(path_restore_model,\n",
    "                                                         variables_to_restore)\n",
    "                init_fn(sess)\n",
    "                logger.info('Restoring parameters from ' +\n",
    "                            '{}'.format(path_restore_model))\n",
    "\n",
    "        # Adds a Graph to the event file.\n",
    "        # create summary that give output (TensorFlow op that output protocol \n",
    "        # buffers containing 'summarized' data) of the built Tensorflow graph:\n",
    "        summary_writer.add_graph(sess.graph)\n",
    "\n",
    "        # start timer for total training time:\n",
    "        start_total_time = time.time()\n",
    "        # set timer to measure the displayed training steps:\n",
    "        start_time = start_total_time\n",
    "\n",
    "        ########################################################################\n",
    "        \n",
    "        # Training:\n",
    "        # train loop\n",
    "        # train until all data is processed (queue empty),\n",
    "        # number of iterations depends on number of data, number of epochs and \n",
    "        # batch size:\n",
    "        iter_start = data_train.iter_left\n",
    "        logger.info('For training it takes {}\\n'.format(iter_start) +\n",
    "                    '    (= # data / batch_size * epochs) iterations to loop ' +\n",
    "                    'through {} samples of\\n  '.format(data_train.df.shape[0]) +\n",
    "                    '  training data over {} '.format(data_train.num_epochs) +\n",
    "                    'epochs summarized in batches of size ' + \n",
    "                    '{}.\\n'.format(data_train.batch_size) +\n",
    "                    '    So, there are # data / batch_size = ' +\n",
    "                    '{}'.format(int(data_train.df.shape[0]/data_train.batch_size))+\n",
    "                    ' iterations per epoch.')\n",
    "\n",
    "        while data_train.iter_left >= 0:\n",
    "            try:\n",
    "                # take a (mini) batch of the training data:\n",
    "                deq_train = data_train.dequeue()\n",
    "                img_b, alb_b, shad_b, inv_b = iq.next_batch(deq=deq_train, \n",
    "                                                            output_shape=image_shape,\n",
    "                                                            is_scale=True,\n",
    "                                                            is_flip=True, \n",
    "                                                            is_rotated=True,\n",
    "                                                            norm=norm)\n",
    "                # run training/optimization step:\n",
    "                # Run one step of the model.  The return values are the \n",
    "                # activations from the `train_op` (which is discarded) and the \n",
    "                # `loss` Op.  To inspect the values of your Ops or variables, \n",
    "                # you may include them in the list passed to sess.run() and the \n",
    "                # value tensors will be returned in the tuple from the call.\n",
    "                feed_dict_tr = {x: img_b,\n",
    "                                y_albedo_label: alb_b,\n",
    "                                y_shading_label: shad_b,\n",
    "                                invalid_px_mask: inv_b,\n",
    "                                training: True}\n",
    "                sess.run(opt_step, feed_dict=feed_dict_tr)\n",
    "\n",
    "                ################################################################\n",
    "\n",
    "                # report training set accuracy every display_step-th step:\n",
    "                if (data_train.num_iter) % display_step == 0:\n",
    "                    # console output:\n",
    "                    train_loss_dict = {}\n",
    "                    for key, val in loss_dict.items():\n",
    "                        train_loss_dict[key] = sess.run(val, \n",
    "                                                        feed_dict=feed_dict_tr)\n",
    "\n",
    "                    dur_time = time.time() - start_time\n",
    "                    dur_time = ghelp.get_time_format(time_in_sec=dur_time)\n",
    "                    dur_time = ghelp.time_tuple_to_str(time_tuple=dur_time)\n",
    "                    logger.info('step {}: '.format(data_train.num_iter) +\n",
    "                                'training ({}) loss '.format(loss_opt) +\n",
    "                                '{:.4f}'.format(train_loss_dict[loss_opt]) + \n",
    "                                '\\n    (' +\n",
    "                                ', '.join(['{}: {:.4f}'.format(it[0], it[1]) \\\n",
    "                                           for it in train_loss_dict.items() \\\n",
    "                                           if it[0]!=loss_opt]) +\n",
    "                                ' , ET: {}).'.format(dur_time))\n",
    "                    # reset timer to measure the displayed training steps:\n",
    "                    start_time = time.time()\n",
    "\n",
    "                ################################################################\n",
    "\n",
    "                # Validation:\n",
    "                # display validation set accuracy and loss after each completed \n",
    "                # epoch (1 epoch ^= data_train.df.shape[0]/data_train.batch_size\n",
    "                # training steps => steps per epoch)\n",
    "                val_epoch = int(data_train.df.shape[0] / data_train.batch_size)\n",
    "                \n",
    "                if data_train.num_iter % val_epoch == 0:\n",
    "                    # After each training epoch we will use the complete \n",
    "                    # validation data set to calculate the error/accuracy on the\n",
    "                    # validation set:\n",
    "                    # loop through one validation data set epoch:\n",
    "                    # initialize dictionary which contains all validation \n",
    "                    # losses:\n",
    "                    valid_loss_dict = dict.fromkeys(loss_dict, 0)\n",
    "                    valid_steps_per_epoch = int(data_valid.df.shape[0] / \n",
    "                                                data_valid.batch_size)\n",
    "                    for j in range(valid_steps_per_epoch):\n",
    "                        # DISCLAIMER: we do not run the opt_step here (on \n",
    "                        # the validation data set) because we do not want to \n",
    "                        # train our network on the validation set. Important \n",
    "                        # for batch normalization and dropout \n",
    "                        # (training -> False).\n",
    "                        # get validation data set (mini) batch:\n",
    "                        lst = iq.next_batch(deq=data_valid.dequeue(), \n",
    "                                            output_shape=image_shape,\n",
    "                                            is_scale=False,\n",
    "                                            is_flip=False,\n",
    "                                            is_rotated=False,\n",
    "                                            norm=norm)\n",
    "                        img_b_val, alb_b_val, shad_b_val, inv_b_val = lst\n",
    "\n",
    "                        # calculate the mean loss of this validation batch and \n",
    "                        # sum it with the previous mean batch losses:\n",
    "                        fd_val = {x: img_b_val,\n",
    "                                         y_albedo_label: alb_b_val,\n",
    "                                         y_shading_label: shad_b_val,\n",
    "                                         invalid_px_mask: inv_b_val,\n",
    "                                         training: False}\n",
    "\n",
    "                        for key, val in loss_dict.items():\n",
    "                            # divide each loss loss by the iteration steps \n",
    "                            # (steps_per_epoch) to get the mean val loss:\n",
    "                            mean_val = val / valid_steps_per_epoch\n",
    "                            valid_loss_dict[key] += sess.run(mean_val, \n",
    "                                                             feed_dict=fd_val)\n",
    "                    # adding a mean loss summary op (for tensorboard):\n",
    "                    feed_dict_vl = {valid_dict[key]: valid_loss_dict[key] for \\\n",
    "                                    key in valid_dict.keys()}\n",
    "                    val_loss_sums = sess.run(valid_sums_merged,\n",
    "                                             feed_dict=feed_dict_vl)\n",
    "                    summary_writer.add_summary(summary=val_loss_sums, \n",
    "                                               global_step=data_train.num_iter)\n",
    "\n",
    "                    dur_time = time.time() - start_time\n",
    "                    dur_time = ghelp.get_time_format(time_in_sec=dur_time)\n",
    "                    dur_time = ghelp.time_tuple_to_str(time_tuple=dur_time)\n",
    "                    logger.info('step {} '.format(data_train.num_iter) +\n",
    "                                '(epoch ' + \n",
    "                                '{}):'.format(data_train.completed_epochs + 1) +\n",
    "                                ' mean validation losses:\\n    ' +\n",
    "                                ', '.join(['{}: {:.4f}'.format(it[0], it[1]) \\\n",
    "                                           for it in valid_loss_dict.items()]) +\n",
    "                                ' (ET: {}).'.format(dur_time))\n",
    "                    # reset timer to measure the displayed training steps:\n",
    "                    start_time = time.time()\n",
    "\n",
    "                ################################################################\n",
    "\n",
    "                if data_train.num_iter % save_step == 0:\n",
    "                    # save checkpoint files to disk:\n",
    "                    save_path = saver.save(sess, log_dir + 'tfmodel',\n",
    "                                           global_step=data_train.num_iter)\n",
    "                    s = sess.run(merge_train_summaries, feed_dict=feed_dict_tr)\n",
    "                    # adds a Summary protocol buffer to the event file \n",
    "                    # (global_step: Number. Optional global step value to record\n",
    "                    # with the summary. Each stepp i is assigned to the \n",
    "                    # corresponding summary parameter.)\n",
    "                    summary_writer.add_summary(summary=s, \n",
    "                                               global_step=data_train.num_iter)\n",
    "                    dur_time = time.time() - start_time\n",
    "                    dur_time = ghelp.get_time_format(time_in_sec=dur_time)\n",
    "                    dur_time = ghelp.time_tuple_to_str(time_tuple=dur_time)\n",
    "                    logger.info('Saved data (step ' + \n",
    "                                '{}):\\n'.format(data_train.num_iter) +\n",
    "                                '    Checkpoint file written to: ' + \n",
    "                                '{} '.format(save_path) +\n",
    "                                '(ET: {}).'.format(dur_time))\n",
    "                    # reset timer to measure the displayed training steps:\n",
    "                    start_time = time.time()\n",
    "\n",
    "            # end while loop when there are no elements left to dequeue:\n",
    "            except IndexError:\n",
    "                end_total_time = time.time() - start_total_time\n",
    "                end_total_time = ghelp.get_time_format(end_total_time)\n",
    "                end_total_time = ghelp.time_tuple_to_str(time_tuple=end_total_time)\n",
    "                logger.info('Training done... total training time: ' + \n",
    "                            '{}.'.format(end_total_time))\n",
    "                break\n",
    "\n",
    "    logger.info('Finished training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If not available download vgg16 ckpt files to ./models/slim/checkpoints\n",
      "Data has apparently already been downloaded and unpacked.\n"
     ]
    }
   ],
   "source": [
    "m_height = 13  # multiplicate of image height size -> network is designed so \n",
    "    # that it can take images with shape of multiples of m\n",
    "m_width = m_height  # multiplicate of image width size -> network \n",
    "    # is designed so that it can take images with shape of multiples of m\n",
    "\n",
    "\n",
    "# # narihira2015:\n",
    "# nodes_name_dict = {'input': 'input:0',\n",
    "#                    'output_albedo': 'deconv_s2out_albedo/BiasAdd:0',\n",
    "#                    'output_shading': 'deconv_s2out_shading/BiasAdd:0'}\n",
    "\n",
    "# params = {'log_dir': 'logs/narihira2015/test',  # path to summary files\n",
    "#           'data_dir': '/usr/udo/data/',\n",
    "#           'path_inference_graph': 'models/narihira2015/tfmodel_inference.meta',\n",
    "#           'path_restore_model': None, # e.g. 'logs/2/tfmodel-5' or None\n",
    "#           'restore_scope': None,\n",
    "#           'image_shape': [32 * m_height, 32 * m_width, 3],\n",
    "#           'initial_learning_rate': 5e-4,  # hyper param\n",
    "#           'loss_opt': 'berhu',\n",
    "#           'batch_size': 16,  # hyper param\n",
    "#           'num_epochs': 50,  # hyper param \n",
    "#           'display_step': 20,\n",
    "#           'save_step': 100,\n",
    "#           'nodes_name_dict': nodes_name_dict,\n",
    "#           'norm': False,\n",
    "#           'plot_inference_graph': False,\n",
    "#           'is_sample_size': False\n",
    "#          }\n",
    "\n",
    "\n",
    "# slim_vgg16_narihira2015:\n",
    "nodes_name_dict = {'input': 'input:0',\n",
    "                   'output_albedo': 'scale2/deconv6_s2_albedo/BiasAdd:0',\n",
    "                   'output_shading': 'scale2/deconv6_s2_shading/BiasAdd:0'}\n",
    "# download checkpoint files:\n",
    "url = \"http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz\"\n",
    "checkpoints_dir = './models/slim/checkpoints'\n",
    "print('If not available download vgg16 ckpt files to ' + checkpoints_dir)\n",
    "download.maybe_download_and_extract(url=url, \n",
    "                                    download_dir=checkpoints_dir,\n",
    "                                    print_download_progress=True)\n",
    "params = {'log_dir': 'logs/slim_vgg16_narihira2015/test/',\n",
    "          'data_dir': '/usr/udo/data/',\n",
    "          'path_inference_graph': 'models/slim/graphs/vgg16_narihira2015/tfmodel_inference.meta',\n",
    "          'path_restore_model': 'models/slim/checkpoints/vgg_16.ckpt',\n",
    "          'restore_scope': 'vgg_16',\n",
    "          'image_shape': [32 * m_height, 32 * m_width, 3],\n",
    "          'initial_learning_rate': 5e-4,  # hyper param\n",
    "          'loss_opt': 'berhu',\n",
    "          'batch_size': 16,  # hyper param\n",
    "          'num_epochs': 100,  # hyper param \n",
    "          'display_step': 20,\n",
    "          'save_step': 100,\n",
    "          'nodes_name_dict': nodes_name_dict,\n",
    "          'norm': False,\n",
    "          'plot_inference_graph': False,\n",
    "          'is_sample_size': False\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-14 17:45:52 DEBUG]: Python version: \n",
      "    3.5.0+ (default, Oct 11 2015, 09:05:38) \n",
      "[GCC 5.2.1 20151010]\n",
      "    Tensorflow version: 1.2.0\n",
      "[2017-09-14 17:45:52 INFO]: Training on images of shape: [416, 416, 3]\n",
      "[2017-09-14 17:45:52 INFO]: Initial learning rate: 0.0005\n",
      "[2017-09-14 17:45:52 INFO]: Loss function used for optimization: berhu\n",
      "[2017-09-14 17:45:52 INFO]: Batch size: 16\n",
      "[2017-09-14 17:45:52 INFO]: # epochs: 100\n",
      "[2017-09-14 17:45:52 INFO]: Report training set loss every 20iteration.\n",
      "[2017-09-14 17:45:52 INFO]: Write summary and checkpoints to file every 100-th iteration.\n",
      "[2017-09-14 17:45:52 DEBUG]: Restored inference graph from\n",
      "    models/slim/graphs/vgg16_narihira2015/tfmodel_inference.meta\n",
      "[2017-09-14 17:45:52 INFO]: # of parameters that can be restored: 26.\n",
      "[2017-09-14 17:45:52 DEBUG]: Defined losses.\n",
      "[2017-09-14 17:45:53 DEBUG]: Defined optimization method.\n",
      "[2017-09-14 17:45:53 DEBUG]: Defined validation losses.\n",
      "[2017-09-14 17:45:53 DEBUG]: Finished building training graph.\n",
      "[2017-09-14 17:45:53 INFO]: Total parameters of network: 49005222\n",
      "[2017-09-14 17:45:54 DEBUG]: Imported training data (#: 690) from\n",
      "    /usr/udo/data/data_sintel_shading_train.csv\n",
      "[2017-09-14 17:45:54 DEBUG]: Imported validation data (#: 135) from\n",
      "    /usr/udo/data/data_sintel_shading_valid.csv\n",
      "[2017-09-14 17:45:54 INFO]: Start training:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/slim/checkpoints/vgg_16.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/slim/checkpoints/vgg_16.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-14 17:46:03 INFO]: Restoring parameters from models/slim/checkpoints/vgg_16.ckpt\n",
      "[2017-09-14 17:46:03 INFO]: For training it takes 4300\n",
      "    (= # data / batch_size * epochs) iterations to loop through 690 samples of\n",
      "    training data over 100 epochs summarized in batches of size 16.\n",
      "    So, there are # data / batch_size = 43 iterations per epoch.\n"
     ]
    }
   ],
   "source": [
    "train_network(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-25T14:42:31.406731Z",
     "start_time": "2017-08-25T14:42:31.382090Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !tensorboard --logdir /logs/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0+"
  },
  "notify_time": "5",
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
